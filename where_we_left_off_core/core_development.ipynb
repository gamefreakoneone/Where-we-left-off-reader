{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a04b39",
   "metadata": {},
   "source": [
    "# This is Chapter exrtaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89655538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import fitz  \n",
    "FRONT_MATTER_STOPWORDS = {\n",
    "    \"contents\", \"table of contents\", \"copyright\", \"title page\",\n",
    "    \"about the author\", \"dedication\", \"preface\", \"foreword\", \"acknowledgements\",\n",
    "}\n",
    "\n",
    "def is_probable_chapter(title: str) -> bool:\n",
    "    t = title.strip().lower()\n",
    "    if t in FRONT_MATTER_STOPWORDS:\n",
    "        return False\n",
    "\n",
    "    if re.match(r\"^\\s*(chapter\\s+\\d+|[ivxlcdm]+\\.)\\b\", t):  # \"Chapter 1\" or Roman numerals\n",
    "        return True\n",
    "    if re.match(r\"^\\s*\\d+(\\.\\d+)*\\b\", t):  # \"1\", \"1.2\", \"12.3.4\"\n",
    "        return True\n",
    "    \n",
    "    return len(title.strip()) >= 6\n",
    "\n",
    "def load_toc(doc: fitz.Document):\n",
    "    toc = doc.get_toc(simple=True)  # [[level, title, page], ...]\n",
    "    clean = []\n",
    "    for level, title, page in toc:\n",
    "        if page is None or page < 1:\n",
    "            continue  # skip missing/external links\n",
    "        clean.append((level, title.strip(), page))\n",
    "    return clean\n",
    "\n",
    "def derive_chapter_ranges(doc: fitz.Document):\n",
    "    \"\"\"\n",
    "    From full TOC -> list of canonical chapter entries with [start,end] page indices (0-based).\n",
    "    Rule: chapter ends at the page before the next TOC item whose level <= this level.\n",
    "    \"\"\"\n",
    "    toc = load_toc(doc)\n",
    "    if not toc:\n",
    "        return []\n",
    "\n",
    "    level_counts = {}\n",
    "    for level, title, _ in toc:\n",
    "        if is_probable_chapter(title):\n",
    "            level_counts[level] = level_counts.get(level, 0) + 1\n",
    "    chapter_level = min(level_counts, key=lambda k: ( -level_counts[k], k )) if level_counts else 1\n",
    "\n",
    "    filtered = [(lvl, title, page) for (lvl, title, page) in toc if lvl >= chapter_level]\n",
    "\n",
    "    chapters = []\n",
    "    for i, (lvl, title, page1_based) in enumerate(filtered):\n",
    "        if lvl != chapter_level:\n",
    "            continue\n",
    "        start = max(0, page1_based - 1) \n",
    "\n",
    "\n",
    "        end_0based = doc.page_count - 1\n",
    "        for j in range(i + 1, len(filtered)):\n",
    "            lvl_j, _, page_j_1b = filtered[j]\n",
    "            if lvl_j <= chapter_level:\n",
    "                end_0based = max(0, page_j_1b - 2) \n",
    "                break\n",
    "\n",
    "        if start <= end_0based:\n",
    "            chapters.append({\n",
    "                \"level\": lvl,\n",
    "                \"title\": title,\n",
    "                \"start_page\": start,\n",
    "                \"end_page\": end_0based\n",
    "            })\n",
    "\n",
    "    chapters = [c for c in chapters if is_probable_chapter(c[\"title\"])]\n",
    "    return chapters\n",
    "\n",
    "def extract_text_range(doc: fitz.Document, start_page: int, end_page: int) -> str:\n",
    "    parts = []\n",
    "    for p in range(start_page, end_page + 1):\n",
    "        page = doc[p]\n",
    "        parts.append(page.get_text(\"text\", sort=True))\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "# # ---- Example usage ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     path = r\"C:\\Users\\abiju\\Desktop\\Project-Velcro\\REference_textbook\\Charlie and the Chocolate Factory (Roald Dahl).pdf\"\n",
    "#     doc = fitz.open(path)\n",
    "#     chapters = derive_chapter_ranges(doc)\n",
    "\n",
    "#     for idx, ch in enumerate(chapters[:5]):\n",
    "#         print(f\"[{idx}] {ch['title']}  -> pages {ch['start_page']+1}–{ch['end_page']+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921f729",
   "metadata": {},
   "source": [
    "# First pass JSON structure\n",
    "\n",
    "```jsonc\n",
    "{\n",
    "  \"chapter\": [{\n",
    "    \"chapter_id\": 0,                // Chapter number\n",
    "    \"title\": \"\",                    // Chapter title\n",
    "    \"pages\": [0, 0],                // [start_page, end_page]\n",
    "    \"summary_local\": \"\",            // <= 160 words\n",
    "    \"characters\": [\n",
    "      {\n",
    "        \"name\": \"\",                 // Character name\n",
    "        \"aliases\": [],\n",
    "        \"status\": \"\",               // Character status\n",
    "        \"chapter_role\": \"\",         // Role in this chapter\n",
    "        \"character_actions\": \"\",\n",
    "        \"relationships\": [\n",
    "          {\n",
    "            \"with_name\": \"\",        // Other character's name\n",
    "            \"type\": \"\",             // Relationship type\n",
    "            \"justification\": \"\"     // <= 30 words\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  } \n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "374303bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chapter_skeletons(chapters):\n",
    "    \"\"\"\n",
    "    chapters: list of dicts with keys title, start_page, end_page\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for chapter_id, ch in enumerate(chapters, start=1):\n",
    "        out.append({\n",
    "            \"chapter_id\": chapter_id,\n",
    "            \"title\": ch[\"title\"],\n",
    "            \"pages\": [ch[\"start_page\"], ch[\"end_page\"]],\n",
    "            \"summary_local\": \"\",\n",
    "            \"characters\": [\n",
    "                {\n",
    "                    \"name\": \"\",\n",
    "                    \"aliases\": [],\n",
    "                    \"status\": \"\",\n",
    "                    \"chapter_role\": \"\",\n",
    "                    \"character_actions\": \"\",\n",
    "                    \"relationships\": [\n",
    "                        {\"with_name\": \"\", \"type\": \"\", \"justification\": \"\"}\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        })\n",
    "    return {\"chapter\": out}\n",
    "\n",
    "# skeleton = make_chapter_skeletons(chapters)\n",
    "# print(json.dumps(skeleton, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d75dd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc601e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI, RateLimitError, APIError, APIConnectionError, InternalServerError\n",
    "import asyncio\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    with_name: str = Field(..., description=\"Other character's name\")\n",
    "    type: str = Field(..., description=\"Relationship type (e.g., ally | mentor | antagonist | family | rival | colleague | unknown)\")\n",
    "    justification: str = Field(..., description=\"<= 50 words\")\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the character\")\n",
    "    aliases: List[str] = Field(..., description=\"List of aliases for the character\")\n",
    "    status: str = Field(..., description=\"e.g: active | missing | dead | resolved | tentative\")\n",
    "    chapter_role: str = Field(..., description=\"e.g: POV | supporting | antagonist | cameo | unknown\")\n",
    "    character_actions: str = Field(..., description=\"Key actions or events involving the character in this chapter. Be descriptive and use short hand (<= 100 words)\")\n",
    "    relationships: List[Relationship]\n",
    "\n",
    "class ChapterFill(BaseModel):\n",
    "    summary_local: str = Field(..., description=\"summary of the chapter in <= 160 words\")\n",
    "    characters: List[Character]\n",
    "\n",
    "chapter_fill_prompt = \"You are a library assistant who is skilled at extracting structured information from story book chapters. You are given the text of a chapter and must fill in the structured data fields, such that it meets the following criteria:\" \\\n",
    "\"1. The summary_local field must contain a concise summary of the chapter in the context of the data provided. Even if you are aware about the story you are dealing with, do not add additional information that can potentially spoil the future chapters, limited to 160 words.\\n\" \\\n",
    "\"2. For the characters entry, it is a list of Character objects, each with the following fields:\\n\" \\\n",
    "\"   - name: Name of the character\\n\" \\\n",
    "\"   - aliases: List of aliases for the character. Do not include generic pronouns such as 'he', 'she', or 'they' or role words such as teacher, guard , protagonist , antagonist etc\\n\" \\\n",
    "\"   - status: e.g: active | missing | dead | resolved | tentative\\n\" \\\n",
    "\"   - chapter_role: e.g: POV | supporting | antagonist | cameo | unknown\\n\" \\\n",
    "\"   - character_actions: Key actions or events involving the character in this chapter (<= 100 words)\\n\" \\\n",
    "\"   - relationships: List of Relationship objects\\n\" \\\n",
    "\"3. For the relationships entry, it is a list of Relationship objects, each with the following fields:\\n\" \\\n",
    "\"   - with_name: Other character's name\\n\" \\\n",
    "\"   - type: Relationship type (e.g., ally | mentor | antagonist | family | rival | colleague | unknown)\\n\" \\\n",
    "\"   - justification: an explanation in the context of the chapter why the relationship exists ( <= 50 words)\\n\"\n",
    "\n",
    "# def fill_chapter_with_model(chapter_text: str) -> ChapterFill:\n",
    "#     system = chapter_fill_prompt\n",
    "#     resp = client.responses.parse(\n",
    "#         model=\"gpt-5-mini\",\n",
    "#         input=[\n",
    "#             {\"role\": \"system\", \"content\": system},\n",
    "#             {\"role\": \"user\", \"content\": chapter_text},\n",
    "#         ],\n",
    "#         text_format=ChapterFill,\n",
    "#     )\n",
    "#     return resp.output_parsed\n",
    "\n",
    "async def fill_chapter_with_model_async(client: AsyncOpenAI, chapter_text: str, *, semaphore: asyncio.Semaphore,\n",
    "                                        max_retries: int = 5) -> ChapterFill:\n",
    "    backoff = 1\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with semaphore:\n",
    "                resp = await client.responses.parse(\n",
    "                    model=\"gpt-5-mini\",\n",
    "                    input=[\n",
    "                        {\"role\": \"system\", \"content\": chapter_fill_prompt},\n",
    "                        {\"role\": \"user\", \"content\": chapter_text},\n",
    "                    ],\n",
    "                    text_format=ChapterFill,\n",
    "                )\n",
    "            return resp.output_parsed\n",
    "        except (RateLimitError, APIError, APIConnectionError, InternalServerError) as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            await asyncio.sleep(backoff + random.random())\n",
    "            backoff = min(backoff * 2, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91e652c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'level': 2,\n",
       "  'title': '1 Here Comes Charlie',\n",
       "  'start_page': 11,\n",
       "  'end_page': 17},\n",
       " {'level': 2,\n",
       "  'title': \"2 Mr Willy Wonka's Factory\",\n",
       "  'start_page': 18,\n",
       "  'end_page': 21},\n",
       " {'level': 2,\n",
       "  'title': '3 Mr Wonka and the Indian Prince',\n",
       "  'start_page': 22,\n",
       "  'end_page': 24},\n",
       " {'level': 2,\n",
       "  'title': '4 The Secret Workers',\n",
       "  'start_page': 25,\n",
       "  'end_page': 28},\n",
       " {'level': 2,\n",
       "  'title': '5 The Golden Tickets',\n",
       "  'start_page': 29,\n",
       "  'end_page': 30},\n",
       " {'level': 2,\n",
       "  'title': '6 The First Two Finders',\n",
       "  'start_page': 31,\n",
       "  'end_page': 35},\n",
       " {'level': 2,\n",
       "  'title': \"7 Charlie's Birthday\",\n",
       "  'start_page': 36,\n",
       "  'end_page': 38},\n",
       " {'level': 2,\n",
       "  'title': '8 Two More Golden Tickets Found',\n",
       "  'start_page': 39,\n",
       "  'end_page': 42},\n",
       " {'level': 2,\n",
       "  'title': '9 Grandpa Joe Takes a Gamble',\n",
       "  'start_page': 43,\n",
       "  'end_page': 44},\n",
       " {'level': 2,\n",
       "  'title': '10 The Family Begins to Starve',\n",
       "  'start_page': 45,\n",
       "  'end_page': 50},\n",
       " {'level': 2, 'title': '11 The Miracle', 'start_page': 51, 'end_page': 54},\n",
       " {'level': 2,\n",
       "  'title': '12 What It Said on the Golden Ticket',\n",
       "  'start_page': 55,\n",
       "  'end_page': 61},\n",
       " {'level': 2,\n",
       "  'title': '13 The Big Day Arrives',\n",
       "  'start_page': 62,\n",
       "  'end_page': 65},\n",
       " {'level': 2, 'title': '14 Mr Willy Wonka', 'start_page': 66, 'end_page': 70},\n",
       " {'level': 2,\n",
       "  'title': '15 The Chocolate Room',\n",
       "  'start_page': 71,\n",
       "  'end_page': 75},\n",
       " {'level': 2,\n",
       "  'title': '16 The Oompa-Loompas',\n",
       "  'start_page': 76,\n",
       "  'end_page': 78},\n",
       " {'level': 2,\n",
       "  'title': '17 Augustus Gloop Goes up the Pipe',\n",
       "  'start_page': 79,\n",
       "  'end_page': 86},\n",
       " {'level': 2,\n",
       "  'title': '18 Down the Chocolate River',\n",
       "  'start_page': 87,\n",
       "  'end_page': 92},\n",
       " {'level': 2,\n",
       "  'title': '19 The Inventing Room – Everlasting Gobstoppers and Hair Toffee',\n",
       "  'start_page': 93,\n",
       "  'end_page': 97},\n",
       " {'level': 2,\n",
       "  'title': '20 The Great Gum Machine',\n",
       "  'start_page': 98,\n",
       "  'end_page': 99},\n",
       " {'level': 2,\n",
       "  'title': '21 Good-bye Violet',\n",
       "  'start_page': 100,\n",
       "  'end_page': 107},\n",
       " {'level': 2,\n",
       "  'title': '22 Along the Corridor',\n",
       "  'start_page': 108,\n",
       "  'end_page': 111},\n",
       " {'level': 2,\n",
       "  'title': '23 Square Sweets That Look Round',\n",
       "  'start_page': 112,\n",
       "  'end_page': 114},\n",
       " {'level': 2,\n",
       "  'title': '24 Veruca in the Nut Room',\n",
       "  'start_page': 115,\n",
       "  'end_page': 122},\n",
       " {'level': 2,\n",
       "  'title': '25 The Great Glass Lift',\n",
       "  'start_page': 123,\n",
       "  'end_page': 128},\n",
       " {'level': 2,\n",
       "  'title': '26 The Television-Chocolate Room',\n",
       "  'start_page': 129,\n",
       "  'end_page': 133},\n",
       " {'level': 2,\n",
       "  'title': '27 Mike Teavee is Sent by Television',\n",
       "  'start_page': 134,\n",
       "  'end_page': 145},\n",
       " {'level': 2,\n",
       "  'title': '28 Only Charlie Left',\n",
       "  'start_page': 146,\n",
       "  'end_page': 151},\n",
       " {'level': 2,\n",
       "  'title': '29 The Other Children Go Home',\n",
       "  'start_page': 152,\n",
       "  'end_page': 154},\n",
       " {'level': 2,\n",
       "  'title': \"30 Charlie's Chocolate Factory\",\n",
       "  'start_page': 155,\n",
       "  'end_page': 160}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_all_chapters(doc: fitz.Document, chapters: list) -> dict:\n",
    "#     \"\"\"\n",
    "#     Processes all chapters, fills them with AI-generated data, and returns the final JSON object.\n",
    "#     \"\"\"\n",
    "#     final_data = make_chapter_skeletons(chapters)\n",
    "    \n",
    "#     for i, chapter_info in enumerate(final_data[\"chapter\"]):\n",
    "#         print(f\"Processing Chapter {chapter_info['chapter_id']}: '{chapter_info['title']}'...\")\n",
    "        \n",
    "#         start_page, end_page = chapter_info[\"pages\"]\n",
    "#         chapter_text = extract_text_range(doc, start_page, end_page)\n",
    "        \n",
    "#         if not chapter_text.strip():\n",
    "#             print(f\"  -> No text found for chapter {chapter_info['chapter_id']}. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Get structured data from the AI model\n",
    "#         filled_data = fill_chapter_with_model(chapter_text)\n",
    "        \n",
    "#         # Update the final JSON structure\n",
    "#         final_data[\"chapter\"][i][\"summary_local\"] = filled_data.summary_local\n",
    "#         # Convert Pydantic character models to dictionaries for JSON serialization\n",
    "#         final_data[\"chapter\"][i][\"characters\"] = [char.model_dump() for char in filled_data.characters]\n",
    "\n",
    "#     return final_data\n",
    "\n",
    "\n",
    "async def process_all_chapters_async(doc: fitz.Document, chapters: list, max_concurrency: int = 4) -> dict:\n",
    "    extracted = []\n",
    "    for ch in chapters:\n",
    "        text = extract_text_range(doc, ch[\"start_page\"], ch[\"end_page\"])\n",
    "        extracted.append({\"meta\": ch, \"text\": text})\n",
    "\n",
    "    final_data = make_chapter_skeletons(chapters)\n",
    "\n",
    "    aclient = AsyncOpenAI()\n",
    "    sem = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    tasks = []\n",
    "    index_map = []  \n",
    "    for i, item in enumerate(extracted):\n",
    "        if not item[\"text\"].strip():\n",
    "            continue\n",
    "        print(f\"Processing Chapter {item['meta']['chapter_id']}...\")\n",
    "        tasks.append(asyncio.create_task(\n",
    "            fill_chapter_with_model_async(aclient, item[\"text\"], semaphore=sem)\n",
    "        ))\n",
    "        index_map.append(i)\n",
    "\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    for task_idx, result in enumerate(results):\n",
    "        i = index_map[task_idx]\n",
    "        if isinstance(result, Exception):\n",
    "            # Leave defaults for this chapter (or you could add an \"error\" field)\n",
    "            print(f\"Chapter {final_data['chapter'][i]['chapter_id']} failed: {result}\")\n",
    "            continue\n",
    "\n",
    "        final_data[\"chapter\"][i][\"summary_local\"] = result.summary_local\n",
    "        final_data[\"chapter\"][i][\"characters\"] = [c.model_dump() for c in result.characters]\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4fd08",
   "metadata": {},
   "source": [
    "#### Look into parallel processing for chapter extraction , taking too long. ✅ (Done: 20 minutes to 4 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68501f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PROCESSING ALL CHAPTERS ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- PROCESSING ALL CHAPTERS ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# final_output = process_all_chapters(doc, chapters)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m final_output = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_all_chapters_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchapters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m output_filename = \u001b[33m\"\u001b[39m\u001b[33mfinal_story_output.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_filename, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abiju\\AppData\\Local\\anaconda3\\envs\\lc-academy-env\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# pdf_path = r\"C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Charlie and the Chocolate Factory (Roald Dahl).pdf\"\n",
    "# doc = fitz.open(pdf_path)\n",
    "\n",
    "# chapters = derive_chapter_ranges(doc)\n",
    "# if not chapters:\n",
    "#     print(\"No chapters were derived from the PDF's table of contents.\")\n",
    "# else:\n",
    "#     print(\"\\n--- PROCESSING ALL CHAPTERS ---\")\n",
    "#     # final_output = process_all_chapters(doc, chapters)\n",
    "#     final_output = asyncio.run(process_all_chapters_async(doc, chapters, max_concurrency=4))\n",
    "#     output_filename = \"final_story_output.json\"\n",
    "#     with open(output_filename, \"w\") as f:\n",
    "#         json.dump(final_output, f, indent=2)\n",
    "        \n",
    "#     print(f\"\\nProcessing complete. The final JSON object has been saved to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PROCESSING ALL CHAPTERS ---\n",
      "\n",
      "Processing complete. The final JSON object has been saved to 'final_story_output.json'\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"C:\\Users\\abiju\\Desktop\\Project-Velcro\\REference_textbook\\Charlie and the Chocolate Factory (Roald Dahl).pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "chapters = derive_chapter_ranges(doc)\n",
    "if not chapters:\n",
    "    print(\"No chapters were derived from the PDF's table of contents.\")\n",
    "else:\n",
    "    print(\"\\n--- PROCESSING ALL CHAPTERS ---\")\n",
    "    final_output = await process_all_chapters_async(doc, chapters, max_concurrency=4)\n",
    "\n",
    "    output_filename = \"final_story_output.json\"\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        json.dump(final_output, f, indent=2)\n",
    "\n",
    "    print(f\"\\nProcessing complete. The final JSON object has been saved to '{output_filename}'\")\n",
    "\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022d9ec",
   "metadata": {},
   "source": [
    "# Seconds pass : Now we connect between the chapters\n",
    "\n",
    "```jsonc\n",
    "{\n",
    "  \"chapter\": [{\n",
    "    \"chapter_id\": 0,                // Chapter number\n",
    "    \"title\": \"\",                    // Chapter title\n",
    "    \"pages\": [0, 0],                // [start_page, end_page]\n",
    "    \"summary_local\": \"\",            // <= 160 words \n",
    "    \"summary_global\": \"\",           // concatenated summary across chapters ≤ 250 words\n",
    "    \"characters\": [\n",
    "      {\n",
    "        \"name\": \"\",                 // Character name\n",
    "        \"aliases\": [],              // Concatenated aliases across all the chapters\n",
    "        \"status\": \"\",               // Update the character status as the story and relationships evolve\n",
    "        \"chapter_role\": \"\",         // Current role in the chapter in the context of the ongoing story \n",
    "        \"character_actions\": \"\",  // Key actions or events involving the character as the story is progressing (<= 150 words)\n",
    "        \"relationships\": [\n",
    "          {\n",
    "            \"with_name\": \"\",        // Other character's name\n",
    "            \"type\": \"\",             // Updated Relationship type as the story progresses and the relationships evolve\n",
    "            \"justification\": \"\",    // Updated in the context of the story <= 100 words\n",
    "            \"importance\": 0         // Updated importance level (0-5)\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  } \n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abc6de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "class RelationshipGlobal(BaseModel):\n",
    "    with_name: str\n",
    "    type: str\n",
    "    justification: str = Field(..., description=\"<= 100 words\")\n",
    "    importance: int = Field(..., description=\"0-5 scale\")\n",
    "\n",
    "class CharacterGlobal(BaseModel):\n",
    "    name: str\n",
    "    aliases: List[str]\n",
    "    status: str\n",
    "    chapter_role: str\n",
    "    character_actions: str = Field(..., description=\"Key actions or events involving the character in the ongoing story. Be descriptive and use short hand <= 200 words\")\n",
    "    relationships: List[RelationshipGlobal]\n",
    "\n",
    "class ChapterGlobal(BaseModel):\n",
    "    summary_global: str = Field(..., description=\"summary of the ongoing story <= 250 words\")\n",
    "    characters: List[CharacterGlobal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "931495a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_characters_locally(existing_chars: Dict[str, dict], new_chars: List[dict]) -> Dict[str, dict]:\n",
    "\n",
    "#     for char in new_chars:\n",
    "#         name = char['name']\n",
    "        \n",
    "#         matched_name = None\n",
    "#         for alias in char.get('aliases', []):\n",
    "#             for existing_name in existing_chars:\n",
    "#                 if alias == existing_name or alias in existing_chars[existing_name].get('aliases', []):\n",
    "#                     matched_name = existing_name\n",
    "#                     break\n",
    "#             if matched_name:\n",
    "#                 break\n",
    "        \n",
    "#         key = matched_name or name\n",
    "        \n",
    "#         if key in existing_chars:\n",
    "#             existing_aliases = set(existing_chars[key].get('aliases', []))\n",
    "#             new_aliases = set(char.get('aliases', []))\n",
    "#             existing_chars[key]['aliases'] = list(existing_aliases | new_aliases)\n",
    "            \n",
    "#             existing_chars[key]['status'] = char['status']\n",
    "#             existing_chars[key]['chapter_role'] = char['chapter_role']\n",
    "#             existing_chars[key]['character_actions'] = char['character_actions']\n",
    "#             existing_chars[key]['relationships'] = char['relationships']\n",
    "#         else:\n",
    "#             # Add new character\n",
    "#             existing_chars[key] = char.copy()\n",
    "    \n",
    "#     return existing_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dc3a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_fill_global = \"\"\"\n",
    "You are a library assistant who is skilled at extracting structured information from story book chapters. You are given the text of a chapter and must fill in the structured data fields, such that it meets the following criteria:\n",
    "1. The summary_global field must contain a concise summary of the ongoing story in the context of the previous chapter summary and current chapter summary provided. Even if you are aware about the story you are dealing with, do not add additional information that can potentially spoil the future chapters, limited to 250 words.\n",
    "2. The characters field must include all relevant characters introduced or developed in the chapter, along with their updated attributes. This includes :\n",
    "   - Name: Name of the character\n",
    "   - Aliases: List of character aliases\n",
    "   - Status: Current status of the character in the context of the ongoing story with reference to previous chapters and current chapter\n",
    "   - Chapter role: Role of the character in the chapter in the context of the ongoing story\n",
    "   - Character actions: Key actions or events involving the character in the ongoing story\n",
    "   - Relationships: List of relationships with other characters\n",
    "3. For the relationships field, it is a list of Relationship objects, each with the following fields:\n",
    "    - with_name: Other character's name\n",
    "    - type: Relationship type (e.g., ally | mentor | antagonist | family | rival | colleague | unknown)\n",
    "    - justification: an explanation in the context of the chapter why the relationship exists ( <= 100 words)\n",
    "    - importance: Importance of the relationship on a scale of 0-5, where 0 is negligible and 5 is critical to the story. Update the importance level as the story progresses.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98204cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_global_view(\n",
    "    client: AsyncOpenAI,\n",
    "    previous_summary: str,\n",
    "    previous_characters: dict,   # dict[str, dict] — your cumulative state\n",
    "    current_chapter: dict        # from first pass\n",
    ") -> ChapterGlobal:\n",
    "    \"\"\"\n",
    "    Calls the LLM to get an intelligently merged global view of the story.\n",
    "    Standardized to use the 'responses.parse' API for consistency.\n",
    "    \"\"\"\n",
    "    context = {\n",
    "        \"previous_story_summary\": previous_summary,\n",
    "        \"all_known_characters_so_far\": list(previous_characters.values()),\n",
    "        \"current_chapter_title\": current_chapter[\"title\"],\n",
    "        \"current_chapter_summary\": current_chapter[\"summary_local\"],\n",
    "        \"characters_in_current_chapter\": current_chapter[\"characters\"],\n",
    "    }\n",
    "\n",
    "    resp = await client.responses.parse(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": chapter_fill_global},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(context, indent=2)}\n",
    "        ],\n",
    "        text_format=ChapterGlobal,\n",
    "    )\n",
    "    # Correctly returns the parsed Pydantic model\n",
    "    return resp.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_and_validate_global(ch: ChapterGlobal) -> ChapterGlobal:\n",
    "    \"\"\"\n",
    "    Enforces data invariants on the LLM's output to prevent drift and errors.\n",
    "    This acts as a critical guardrail.\n",
    "    \"\"\"\n",
    "    if not ch or not ch.characters:\n",
    "        return ch\n",
    "\n",
    "    for c in ch.characters:\n",
    "        # Normalize and deduplicate aliases\n",
    "        seen_aliases = set()\n",
    "        unique_aliases = []\n",
    "        for alias in (c.aliases or []):\n",
    "            stripped_alias = (alias or \"\").strip()\n",
    "            if stripped_alias and stripped_alias.lower() not in seen_aliases:\n",
    "                seen_aliases.add(stripped_alias.lower())\n",
    "                unique_aliases.append(stripped_alias)\n",
    "        c.aliases = unique_aliases\n",
    "\n",
    "        # Validate relationships\n",
    "        if not c.relationships:\n",
    "            continue\n",
    "        for r in c.relationships: # Fixing edge case hallucination\n",
    "            r.importance = max(0, min(5, int(r.importance or 0)))\n",
    "            if r.justification and len(r.justification.split()) > 100:\n",
    "                r.justification = \" \".join(r.justification.split()[:100]) + \"...\"\n",
    "    return ch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f84c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def second_pass_processing(client: AsyncOpenAI, first_pass_data: dict) -> dict:\n",
    "    chapters = first_pass_data[\"chapter\"]\n",
    "    final_output = {\"chapter\": []}\n",
    "\n",
    "    # Initialize cumulative state\n",
    "    cumulative_summary = \"\"\n",
    "    cumulative_characters = {}   # {canonical_name: character_dict}\n",
    "    alias_index = {}             # {normalized_alias: canonical_name}\n",
    "\n",
    "    for i, chapter_data in enumerate(chapters, start=1):\n",
    "        print(f\"Second Pass - Processing chapter {i}/{len(chapters)}: {chapter_data['title']}\")\n",
    "\n",
    "        # 1. Get the intelligently merged view from the LLM\n",
    "        global_view = await create_global_view(\n",
    "            client=client,\n",
    "            previous_summary=cumulative_summary,\n",
    "            previous_characters=cumulative_characters,\n",
    "            current_chapter=chapter_data\n",
    "        )\n",
    "\n",
    "        # 2. Apply guardrails: normalize and validate the LLM's output\n",
    "        validated_global_view = _normalize_and_validate_global(global_view)\n",
    "\n",
    "        # 3. Update the cumulative state using the *validated* data\n",
    "        cumulative_summary = validated_global_view.summary_global\n",
    "        \n",
    "        # Reset and rebuild the character dictionary and alias index from the new ground truth\n",
    "        cumulative_characters = {}\n",
    "        alias_index = {}\n",
    "        \n",
    "        for char_model in validated_global_view.characters:\n",
    "            char_dict = char_model.model_dump()\n",
    "            canonical_name = char_model.name\n",
    "            \n",
    "            cumulative_characters[canonical_name] = char_dict\n",
    "            \n",
    "            # Update the alias index for robust tracking\n",
    "            norm_canon_name = canonical_name.strip().casefold()\n",
    "            alias_index[norm_canon_name] = canonical_name\n",
    "            for alias in char_model.aliases:\n",
    "                alias_index[alias.strip().casefold()] = canonical_name\n",
    "\n",
    "        # 4. Append the state *as of this chapter* to the final output\n",
    "        final_output[\"chapter\"].append({\n",
    "            \"chapter_id\": chapter_data[\"chapter_id\"],\n",
    "            \"title\": chapter_data[\"title\"],\n",
    "            \"pages\": chapter_data[\"pages\"],\n",
    "            \"summary_local\": chapter_data[\"summary_local\"],\n",
    "            \"summary_global\": cumulative_summary,\n",
    "            \"characters\": list(cumulative_characters.values()),\n",
    "        })\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dbe6a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STARTING SECOND PASS PROCESSING ---\n",
      "Second Pass - Processing chapter 1/30: 1 Here Comes Charlie\n",
      "Second Pass - Processing chapter 2/30: 2 Mr Willy Wonka's Factory\n",
      "Second Pass - Processing chapter 3/30: 3 Mr Wonka and the Indian Prince\n",
      "Second Pass - Processing chapter 4/30: 4 The Secret Workers\n",
      "Second Pass - Processing chapter 5/30: 5 The Golden Tickets\n",
      "Second Pass - Processing chapter 6/30: 6 The First Two Finders\n",
      "Second Pass - Processing chapter 7/30: 7 Charlie's Birthday\n",
      "Second Pass - Processing chapter 8/30: 8 Two More Golden Tickets Found\n",
      "Second Pass - Processing chapter 9/30: 9 Grandpa Joe Takes a Gamble\n",
      "Second Pass - Processing chapter 10/30: 10 The Family Begins to Starve\n",
      "Second Pass - Processing chapter 11/30: 11 The Miracle\n",
      "Second Pass - Processing chapter 12/30: 12 What It Said on the Golden Ticket\n",
      "Second Pass - Processing chapter 13/30: 13 The Big Day Arrives\n",
      "Second Pass - Processing chapter 14/30: 14 Mr Willy Wonka\n",
      "Second Pass - Processing chapter 15/30: 15 The Chocolate Room\n",
      "Second Pass - Processing chapter 16/30: 16 The Oompa-Loompas\n",
      "Second Pass - Processing chapter 17/30: 17 Augustus Gloop Goes up the Pipe\n",
      "Second Pass - Processing chapter 18/30: 18 Down the Chocolate River\n",
      "Second Pass - Processing chapter 19/30: 19 The Inventing Room – Everlasting Gobstoppers and Hair Toffee\n",
      "Second Pass - Processing chapter 20/30: 20 The Great Gum Machine\n",
      "Second Pass - Processing chapter 21/30: 21 Good-bye Violet\n",
      "Second Pass - Processing chapter 22/30: 22 Along the Corridor\n",
      "Second Pass - Processing chapter 23/30: 23 Square Sweets That Look Round\n",
      "Second Pass - Processing chapter 24/30: 24 Veruca in the Nut Room\n",
      "Second Pass - Processing chapter 25/30: 25 The Great Glass Lift\n",
      "Second Pass - Processing chapter 26/30: 26 The Television-Chocolate Room\n",
      "Second Pass - Processing chapter 27/30: 27 Mike Teavee is Sent by Television\n",
      "Second Pass - Processing chapter 28/30: 28 Only Charlie Left\n",
      "Second Pass - Processing chapter 29/30: 29 The Other Children Go Home\n",
      "Second Pass - Processing chapter 30/30: 30 Charlie's Chocolate Factory\n",
      "\n",
      "Second pass complete! Saved to 'story_global_view.json'\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    try:\n",
    "        with open(\"final_story_output.json\", \"r\") as f:\n",
    "            first_pass_data = json.load(f)\n",
    "\n",
    "        client = AsyncOpenAI()\n",
    "\n",
    "        print(\"\\n--- STARTING SECOND PASS PROCESSING ---\")\n",
    "        second_pass_output = await second_pass_processing(client, first_pass_data)\n",
    "\n",
    "        output_filename = \"story_global_view.json\"\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            json.dump(second_pass_output, f, indent=2)\n",
    "\n",
    "        print(f\"\\nSecond pass complete! Saved to '{output_filename}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'final_story_output.json' not found. Please run the first pass script first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4c570",
   "metadata": {},
   "source": [
    "## TODO: Make a more efficient second pass: Fool around with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa738f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d0f6129",
   "metadata": {},
   "source": [
    "## Building RAG agent which will combine the JSON output we generated and use semantic search\n",
    "\n",
    "#### So this is going to be a chat interface between the user and the RAG agent. RAG agent will take user queries, take meta data about the current page / status of the chapter the user is currently in, and then use that information to generate queries to the underlying document store. It should be capable of generating sub queries to get more granular information from the document store and then stitch together the appropriate reponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73b80ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "file_path = r\"C:\\Users\\abiju\\Desktop\\Project-Velcro\\REference_textbook\\Chocolate.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "104eed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b59f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a18b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c824e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "page_content='I'm so sorry. Good-bye, Mrs Gloop! And Mr Gloop! Good-bye! I'll see you later . . .' \n",
      "  As Mr and Mrs Gloop and their tiny escort  hurried away, the five Oompa-Loompas on the \n",
      "far side of the river suddenly began hopping and dancing about and beating wildly upon a \n",
      "number of very small drums. 'Augustus Gloop!' they chanted. 'Augustus Gloop! Augustus Gloop! \n",
      "Augustus Gloop!' \n",
      "'Grandpa!' cried Charlie. 'Listen to them, Grandpa! What are they doing?' \n",
      "  'Ssshh!' whispered Grandpa Joe. 'I th ink they're going to sing us a song!' \n",
      "  \n",
      "'Augustus Gloop!' chanted the Oompa-Loompas. \n",
      "'Augustus Gloop! Augustus Gloop! \n",
      "The great big greedy nincompoop! \n",
      "How long could we allow this beast \n",
      "To gorge and guzzle, feed and feast \n",
      "On everything he wanted to? \n",
      "Great Scott! It simply wouldn't do! \n",
      "However long this pig might live, \n",
      "We're positive he'd never give \n",
      "Even the smallest bit of fun \n",
      "Or happiness to anyone. \n",
      "So what we do in cases such \n",
      "As this, we use the gentle touch,' metadata={'producer': 'Acrobat Distiller 7.0 (Windows)', 'creator': 'Acrobat PDFMaker 7.0 for Word', 'creationdate': '2006-08-18T20:56:05-07:00', 'author': 'Aung Myo Min', 'moddate': '2006-08-18T20:56:18-07:00', 'company': 'MediaTech', 'sourcemodified': 'D:20060819035535', 'title': 'There are five children in this book:   AUGUSTUS GLOOP A greedy boy   VERUCA SALT A girl who is spoiled by her parents   VIOLET BEAUREGARDE A girl who chews gum all day long   MIKE TEAVEE A boy who does nothing but watch television   and   CHARLIE BUCKET', 'source': 'C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Chocolate.pdf', 'total_pages': 56, 'page': 28, 'page_label': '29', 'start_index': 0}\n",
      "*******************************************************************\n",
      "1\n",
      "page_content='the oars were being pulled by masses of Oompa-Loompas — at least ten of them to each oar. \n",
      "  'This is my private yacht!' cried Mr Wo nka, beaming with pleasure. 'I made her by \n",
      "hollowing out an enormous boiled sweet! Isn't sh e beautiful! See how she comes cutting through \n",
      "the river!' \n",
      "  The gleaming pink boiled-sweet boat glid ed up to the riverbank. One hundred Oompa-\n",
      "Loompas rested on their oars and stared up at the visitors. Then suddenly, for some reason best \n",
      "known to themselves, they all burst into shrieks of laughter. \n",
      "  'What's so funny?' asked Violet Beauregarde. \n",
      "  'Oh, don't worry about them!' cried Mr Wonka. 'They're always laughing! They think \n",
      "everything's a colossal joke! Jump into the boat, all of you! Come on! Hurry up!' \n",
      "  As soon as everyone was safely in, the Oompa-Loompas pushed the boat away from the \n",
      "bank and began to row swiftly downriver. \n",
      "  'Hey, there! Mike Teavee!' shouted Mr Wo nka. 'Please do not lick the boat with your' metadata={'producer': 'Acrobat Distiller 7.0 (Windows)', 'creator': 'Acrobat PDFMaker 7.0 for Word', 'creationdate': '2006-08-18T20:56:05-07:00', 'author': 'Aung Myo Min', 'moddate': '2006-08-18T20:56:18-07:00', 'company': 'MediaTech', 'sourcemodified': 'D:20060819035535', 'title': 'There are five children in this book:   AUGUSTUS GLOOP A greedy boy   VERUCA SALT A girl who is spoiled by her parents   VIOLET BEAUREGARDE A girl who chews gum all day long   MIKE TEAVEE A boy who does nothing but watch television   and   CHARLIE BUCKET', 'source': 'C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Chocolate.pdf', 'total_pages': 56, 'page': 29, 'page_label': '30', 'start_index': 1523}\n",
      "*******************************************************************\n",
      "2\n",
      "page_content='and Grandpa Joe, stood absolutely still and watched them go. \n",
      "'Listen!' whispered Charlie. 'Listen, Grandpa!  The Oompa-Loompas in the boat outside are \n",
      "starting to sing!' \n",
      "  The voices, one hundred of them singing to gether, came loud and clear into the room: \n",
      "  \n",
      "'Dear friends, we surely all agree \n",
      "There's almost nothing worse to see \n",
      "Than some repulsive little bum \n",
      "Who's always chewing chewing-gum. \n",
      "(It's very near as bad as those' metadata={'producer': 'Acrobat Distiller 7.0 (Windows)', 'creator': 'Acrobat PDFMaker 7.0 for Word', 'creationdate': '2006-08-18T20:56:05-07:00', 'author': 'Aung Myo Min', 'moddate': '2006-08-18T20:56:18-07:00', 'company': 'MediaTech', 'sourcemodified': 'D:20060819035535', 'title': 'There are five children in this book:   AUGUSTUS GLOOP A greedy boy   VERUCA SALT A girl who is spoiled by her parents   VIOLET BEAUREGARDE A girl who chews gum all day long   MIKE TEAVEE A boy who does nothing but watch television   and   CHARLIE BUCKET', 'source': 'C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Chocolate.pdf', 'total_pages': 56, 'page': 35, 'page_label': '36', 'start_index': 3083}\n",
      "*******************************************************************\n",
      "3\n",
      "page_content='chocolate-coated Gloop! No one would buy it.' \n",
      "  'They most certainly would! ' cried Mr Gloop indignantly. \n",
      "  'I don't want to think about it!' shrieked Mrs Gloop. \n",
      "  'Nor do I,' said Mr Wonka. 'And I do pr omise you, madam, that your darling boy is \n",
      "perfectly safe.' \n",
      "  'If he's perfectly safe, then where is he?' snapped Mrs Gloop. 'Lead me to him this instant!' \n",
      "  Mr Wonka turned around and clicked his fing ers sharply, click, click, click, three times. \n",
      "Immediately, an Oompa-Loompa appeared, as if from nowhere, and stood beside him. \n",
      "  The Oompa-Loompa bowed and smiled, showin g beautiful white teeth.  His skin was rosy-\n",
      "white, his long hair was golden-brown, and the top of his head came just above the height of Mr \n",
      "Wonka's knee. He wore the usual deerskin slung over his shoulder. \n",
      "  'Now listen to me!' said Mr Wonka, looking do wn at the tiny man. 'I want you to take Mr \n",
      "and Mrs Gloop up to the Fudge Room and help them to find their son, Augustus. He's just gone \n",
      "up the pipe.'' metadata={'producer': 'Acrobat Distiller 7.0 (Windows)', 'creator': 'Acrobat PDFMaker 7.0 for Word', 'creationdate': '2006-08-18T20:56:05-07:00', 'author': 'Aung Myo Min', 'moddate': '2006-08-18T20:56:18-07:00', 'company': 'MediaTech', 'sourcemodified': 'D:20060819035535', 'title': 'There are five children in this book:   AUGUSTUS GLOOP A greedy boy   VERUCA SALT A girl who is spoiled by her parents   VIOLET BEAUREGARDE A girl who chews gum all day long   MIKE TEAVEE A boy who does nothing but watch television   and   CHARLIE BUCKET', 'source': 'C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Chocolate.pdf', 'total_pages': 56, 'page': 27, 'page_label': '28', 'start_index': 1577}\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"What did the Oompa Loompas sing when Gloop fell into the river?\"\n",
    ")\n",
    "\n",
    "i =0\n",
    "for r in results:\n",
    "    print(i)\n",
    "    i+=1\n",
    "    print(r)\n",
    "    print(\"*******************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57559b9e",
   "metadata": {},
   "source": [
    "## Building the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class UserSearch(MessagesState):\n",
    "    user_page_number: int # The current page the user is on\n",
    "    context_object : Dict[str, Any] # The context object which contains the chapter information and relationships\n",
    "\n",
    "def get_context(state: UserSearch) -> Dict[str, Any]:\n",
    "    \n",
    "    return state.context_object\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
