{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a04b39",
   "metadata": {},
   "source": [
    "# This is Chapter exrtaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89655538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Chapter 1 – Jeeves Exerts the Old Cerebellum  -> pages 2–5\n",
      "[1] Chapter 2 – No Wedding Bells for Bingo  -> pages 6–10\n",
      "[2] Chapter 3 – Aunt Agatha Speaks her Mind  -> pages 11–13\n",
      "[3] Chapter 4 – Pearls Mean Tears  -> pages 14–19\n",
      "[4] Chapter 5 – The Pride of the Woosters is Wounded  -> pages 20–24\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import fitz  \n",
    "FRONT_MATTER_STOPWORDS = {\n",
    "    \"contents\", \"table of contents\", \"copyright\", \"title page\",\n",
    "    \"about the author\", \"dedication\", \"preface\", \"foreword\", \"acknowledgements\",\n",
    "}\n",
    "\n",
    "def is_probable_chapter(title: str) -> bool:\n",
    "    t = title.strip().lower()\n",
    "    if t in FRONT_MATTER_STOPWORDS:\n",
    "        return False\n",
    "\n",
    "    if re.match(r\"^\\s*(chapter\\s+\\d+|[ivxlcdm]+\\.)\\b\", t):  # \"Chapter 1\" or Roman numerals\n",
    "        return True\n",
    "    if re.match(r\"^\\s*\\d+(\\.\\d+)*\\b\", t):  # \"1\", \"1.2\", \"12.3.4\"\n",
    "        return True\n",
    "    \n",
    "    return len(title.strip()) >= 6\n",
    "\n",
    "def load_toc(doc: fitz.Document):\n",
    "    toc = doc.get_toc(simple=True)  # [[level, title, page], ...]\n",
    "    clean = []\n",
    "    for level, title, page in toc:\n",
    "        if page is None or page < 1:\n",
    "            continue  # skip missing/external links\n",
    "        clean.append((level, title.strip(), page))\n",
    "    return clean\n",
    "\n",
    "def derive_chapter_ranges(doc: fitz.Document):\n",
    "    \"\"\"\n",
    "    From full TOC -> list of canonical chapter entries with [start,end] page indices (0-based).\n",
    "    Rule: chapter ends at the page before the next TOC item whose level <= this level.\n",
    "    \"\"\"\n",
    "    toc = load_toc(doc)\n",
    "    if not toc:\n",
    "        return []\n",
    "\n",
    "    level_counts = {}\n",
    "    for level, title, _ in toc:\n",
    "        if is_probable_chapter(title):\n",
    "            level_counts[level] = level_counts.get(level, 0) + 1\n",
    "    chapter_level = min(level_counts, key=lambda k: ( -level_counts[k], k )) if level_counts else 1\n",
    "\n",
    "    filtered = [(lvl, title, page) for (lvl, title, page) in toc if lvl >= chapter_level]\n",
    "\n",
    "    chapters = []\n",
    "    for i, (lvl, title, page1_based) in enumerate(filtered):\n",
    "        if lvl != chapter_level:\n",
    "            continue\n",
    "        start = max(0, page1_based - 1) \n",
    "\n",
    "\n",
    "        end_0based = doc.page_count - 1\n",
    "        for j in range(i + 1, len(filtered)):\n",
    "            lvl_j, _, page_j_1b = filtered[j]\n",
    "            if lvl_j <= chapter_level:\n",
    "                end_0based = max(0, page_j_1b - 2) \n",
    "                break\n",
    "\n",
    "        if start <= end_0based:\n",
    "            chapters.append({\n",
    "                \"level\": lvl,\n",
    "                \"title\": title,\n",
    "                \"start_page\": start,\n",
    "                \"end_page\": end_0based\n",
    "            })\n",
    "\n",
    "    chapters = [c for c in chapters if is_probable_chapter(c[\"title\"])]\n",
    "    return chapters\n",
    "\n",
    "def extract_text_range(doc: fitz.Document, start_page: int, end_page: int) -> str:\n",
    "    parts = []\n",
    "    for p in range(start_page, end_page + 1):\n",
    "        page = doc[p]\n",
    "        parts.append(page.get_text(\"text\", sort=True))\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "# ---- Example usage ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     path = r\"C:\\Users\\amogh\\Desktop\\Project-Velcro\\Books\\The Inimitable Jeeves (P. G. Wodehouse) (Z-Library).pdf\"\n",
    "#     doc = fitz.open(path)\n",
    "#     chapters = derive_chapter_ranges(doc)\n",
    "\n",
    "#     for idx, ch in enumerate(chapters[:5]):\n",
    "#         print(f\"[{idx}] {ch['title']}  -> pages {ch['start_page']+1}–{ch['end_page']+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921f729",
   "metadata": {},
   "source": [
    "# First pass JSON structure\n",
    "\n",
    "```jsonc\n",
    "{\n",
    "  \"chapter\": [{\n",
    "    \"chapter_id\": 0,                // Chapter number\n",
    "    \"title\": \"\",                    // Chapter title\n",
    "    \"pages\": [0, 0],                // [start_page, end_page]\n",
    "    \"summary_local\": \"\",            // <= 160 words\n",
    "    \"characters\": [\n",
    "      {\n",
    "        \"name\": \"\",                 // Character name\n",
    "        \"aliases\": [],\n",
    "        \"status\": \"\",               // Character status\n",
    "        \"chapter_role\": \"\",         // Role in this chapter\n",
    "        \"character_actions\": \"\",\n",
    "        \"relationships\": [\n",
    "          {\n",
    "            \"with_name\": \"\",        // Other character's name\n",
    "            \"type\": \"\",             // Relationship type\n",
    "            \"justification\": \"\"     // <= 30 words\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  } \n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374303bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chapter_skeletons(chapters):# Making the chapters strucutre\n",
    "    out = []\n",
    "    for chapter_id, ch in enumerate(chapters, start=1):\n",
    "        out.append({\n",
    "            \"chapter_id\": chapter_id,\n",
    "            \"title\": ch[\"title\"],\n",
    "            \"pages\": [ch[\"start_page\"], ch[\"end_page\"]],\n",
    "            \"summary_local\": \"\",\n",
    "            \"characters\": [\n",
    "                {\n",
    "                    \"name\": \"\",\n",
    "                    \"aliases\": [],\n",
    "                    \"status\": \"\",\n",
    "                    \"chapter_role\": \"\",\n",
    "                    \"character_actions\": \"\",\n",
    "                    \"relationships\": [\n",
    "                        {\"with_name\": \"\", \"type\": \"\", \"justification\": \"\"}\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        })\n",
    "    return {\"chapter\": out}\n",
    "\n",
    "# skeleton = make_chapter_skeletons(chapters)\n",
    "# print(json.dumps(skeleton, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d75dd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc601e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI, RateLimitError, APIError, APIConnectionError, InternalServerError\n",
    "import asyncio\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    with_name: str = Field(..., description=\"Other character's name\")\n",
    "    type: str = Field(..., description=\"Relationship type (e.g., ally | mentor | antagonist | family | rival | colleague | unknown)\")\n",
    "    justification: str = Field(..., description=\"<= 50 words\")\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the character\")\n",
    "    aliases: List[str] = Field(..., description=\"List of aliases for the character\")\n",
    "    status: str = Field(..., description=\"e.g: active | missing | dead | resolved | tentative\")\n",
    "    chapter_role: str = Field(..., description=\"e.g: POV | supporting | antagonist | cameo | unknown\")\n",
    "    character_actions: str = Field(..., description=\"Key actions or events involving the character in this chapter. Be descriptive and use short hand (<= 100 words)\")\n",
    "    relationships: List[Relationship]\n",
    "\n",
    "class ChapterFill(BaseModel):\n",
    "    summary_local: str = Field(..., description=\"summary of the chapter in <= 160 words\")\n",
    "    characters: List[Character]\n",
    "\n",
    "chapter_fill_prompt = \"You are a library assistant who is skilled at extracting structured information from story book chapters. You are given the text of a chapter and must fill in the structured data fields, such that it meets the following criteria:\" \\\n",
    "\"1. The summary_local field must contain a concise summary of the chapter in the context of the data provided. Even if you are aware about the story you are dealing with, do not add additional information that can potentially spoil the future chapters, limited to 160 words.\\n\" \\\n",
    "\"2. For the characters entry, it is a list of Character objects, each with the following fields:\\n\" \\\n",
    "\"   - name: Name of the character\\n\" \\\n",
    "\"   - aliases: List of aliases for the character. Do not include generic pronouns such as 'he', 'she', or 'they' or role words such as teacher, guard , protagonist , antagonist etc\\n\" \\\n",
    "\"   - status: e.g: active | missing | dead | resolved | tentative\\n\" \\\n",
    "\"   - chapter_role: e.g: POV | supporting | antagonist | cameo | unknown\\n\" \\\n",
    "\"   - character_actions: Key actions or events involving the character in this chapter (<= 100 words)\\n\" \\\n",
    "\"   - relationships: List of Relationship objects\\n\" \\\n",
    "\"3. For the relationships entry, it is a list of Relationship objects, each with the following fields:\\n\" \\\n",
    "\"   - with_name: Other character's name\\n\" \\\n",
    "\"   - type: Relationship type (e.g., ally | mentor | antagonist | family | rival | colleague | unknown)\\n\" \\\n",
    "\"   - justification: an explanation in the context of the chapter why the relationship exists ( <= 50 words)\\n\"\n",
    "\n",
    "# def fill_chapter_with_model(chapter_text: str) -> ChapterFill:\n",
    "#     system = chapter_fill_prompt\n",
    "#     resp = client.responses.parse(\n",
    "#         model=\"gpt-5-mini\",\n",
    "#         input=[\n",
    "#             {\"role\": \"system\", \"content\": system},\n",
    "#             {\"role\": \"user\", \"content\": chapter_text},\n",
    "#         ],\n",
    "#         text_format=ChapterFill,\n",
    "#     )\n",
    "#     return resp.output_parsed\n",
    "\n",
    "async def fill_chapter_with_model_async(client: AsyncOpenAI, chapter_text: str, *, semaphore: asyncio.Semaphore,\n",
    "                                        max_retries: int = 5) -> ChapterFill:\n",
    "    backoff = 1\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with semaphore:\n",
    "                resp = await client.responses.parse(\n",
    "                    model=\"gpt-5-mini\",\n",
    "                    input=[\n",
    "                        {\"role\": \"system\", \"content\": chapter_fill_prompt},\n",
    "                        {\"role\": \"user\", \"content\": chapter_text},\n",
    "                    ],\n",
    "                    text_format=ChapterFill,\n",
    "                )\n",
    "            return resp.output_parsed\n",
    "        except (RateLimitError, APIError, APIConnectionError, InternalServerError) as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            await asyncio.sleep(backoff + random.random())\n",
    "            backoff = min(backoff * 2, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91e652c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'level': 2,\n",
       "  'title': '1 Here Comes Charlie',\n",
       "  'start_page': 11,\n",
       "  'end_page': 17},\n",
       " {'level': 2,\n",
       "  'title': \"2 Mr Willy Wonka's Factory\",\n",
       "  'start_page': 18,\n",
       "  'end_page': 21},\n",
       " {'level': 2,\n",
       "  'title': '3 Mr Wonka and the Indian Prince',\n",
       "  'start_page': 22,\n",
       "  'end_page': 24},\n",
       " {'level': 2,\n",
       "  'title': '4 The Secret Workers',\n",
       "  'start_page': 25,\n",
       "  'end_page': 28},\n",
       " {'level': 2,\n",
       "  'title': '5 The Golden Tickets',\n",
       "  'start_page': 29,\n",
       "  'end_page': 30},\n",
       " {'level': 2,\n",
       "  'title': '6 The First Two Finders',\n",
       "  'start_page': 31,\n",
       "  'end_page': 35},\n",
       " {'level': 2,\n",
       "  'title': \"7 Charlie's Birthday\",\n",
       "  'start_page': 36,\n",
       "  'end_page': 38},\n",
       " {'level': 2,\n",
       "  'title': '8 Two More Golden Tickets Found',\n",
       "  'start_page': 39,\n",
       "  'end_page': 42},\n",
       " {'level': 2,\n",
       "  'title': '9 Grandpa Joe Takes a Gamble',\n",
       "  'start_page': 43,\n",
       "  'end_page': 44},\n",
       " {'level': 2,\n",
       "  'title': '10 The Family Begins to Starve',\n",
       "  'start_page': 45,\n",
       "  'end_page': 50},\n",
       " {'level': 2, 'title': '11 The Miracle', 'start_page': 51, 'end_page': 54},\n",
       " {'level': 2,\n",
       "  'title': '12 What It Said on the Golden Ticket',\n",
       "  'start_page': 55,\n",
       "  'end_page': 61},\n",
       " {'level': 2,\n",
       "  'title': '13 The Big Day Arrives',\n",
       "  'start_page': 62,\n",
       "  'end_page': 65},\n",
       " {'level': 2, 'title': '14 Mr Willy Wonka', 'start_page': 66, 'end_page': 70},\n",
       " {'level': 2,\n",
       "  'title': '15 The Chocolate Room',\n",
       "  'start_page': 71,\n",
       "  'end_page': 75},\n",
       " {'level': 2,\n",
       "  'title': '16 The Oompa-Loompas',\n",
       "  'start_page': 76,\n",
       "  'end_page': 78},\n",
       " {'level': 2,\n",
       "  'title': '17 Augustus Gloop Goes up the Pipe',\n",
       "  'start_page': 79,\n",
       "  'end_page': 86},\n",
       " {'level': 2,\n",
       "  'title': '18 Down the Chocolate River',\n",
       "  'start_page': 87,\n",
       "  'end_page': 92},\n",
       " {'level': 2,\n",
       "  'title': '19 The Inventing Room – Everlasting Gobstoppers and Hair Toffee',\n",
       "  'start_page': 93,\n",
       "  'end_page': 97},\n",
       " {'level': 2,\n",
       "  'title': '20 The Great Gum Machine',\n",
       "  'start_page': 98,\n",
       "  'end_page': 99},\n",
       " {'level': 2,\n",
       "  'title': '21 Good-bye Violet',\n",
       "  'start_page': 100,\n",
       "  'end_page': 107},\n",
       " {'level': 2,\n",
       "  'title': '22 Along the Corridor',\n",
       "  'start_page': 108,\n",
       "  'end_page': 111},\n",
       " {'level': 2,\n",
       "  'title': '23 Square Sweets That Look Round',\n",
       "  'start_page': 112,\n",
       "  'end_page': 114},\n",
       " {'level': 2,\n",
       "  'title': '24 Veruca in the Nut Room',\n",
       "  'start_page': 115,\n",
       "  'end_page': 122},\n",
       " {'level': 2,\n",
       "  'title': '25 The Great Glass Lift',\n",
       "  'start_page': 123,\n",
       "  'end_page': 128},\n",
       " {'level': 2,\n",
       "  'title': '26 The Television-Chocolate Room',\n",
       "  'start_page': 129,\n",
       "  'end_page': 133},\n",
       " {'level': 2,\n",
       "  'title': '27 Mike Teavee is Sent by Television',\n",
       "  'start_page': 134,\n",
       "  'end_page': 145},\n",
       " {'level': 2,\n",
       "  'title': '28 Only Charlie Left',\n",
       "  'start_page': 146,\n",
       "  'end_page': 151},\n",
       " {'level': 2,\n",
       "  'title': '29 The Other Children Go Home',\n",
       "  'start_page': 152,\n",
       "  'end_page': 154},\n",
       " {'level': 2,\n",
       "  'title': \"30 Charlie's Chocolate Factory\",\n",
       "  'start_page': 155,\n",
       "  'end_page': 160}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_all_chapters(doc: fitz.Document, chapters: list) -> dict:\n",
    "#     \"\"\"\n",
    "#     Processes all chapters, fills them with AI-generated data, and returns the final JSON object.\n",
    "#     \"\"\"\n",
    "#     final_data = make_chapter_skeletons(chapters)\n",
    "    \n",
    "#     for i, chapter_info in enumerate(final_data[\"chapter\"]):\n",
    "#         print(f\"Processing Chapter {chapter_info['chapter_id']}: '{chapter_info['title']}'...\")\n",
    "        \n",
    "#         start_page, end_page = chapter_info[\"pages\"]\n",
    "#         chapter_text = extract_text_range(doc, start_page, end_page)\n",
    "        \n",
    "#         if not chapter_text.strip():\n",
    "#             print(f\"  -> No text found for chapter {chapter_info['chapter_id']}. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Get structured data from the AI model\n",
    "#         filled_data = fill_chapter_with_model(chapter_text)\n",
    "        \n",
    "#         # Update the final JSON structure\n",
    "#         final_data[\"chapter\"][i][\"summary_local\"] = filled_data.summary_local\n",
    "#         # Convert Pydantic character models to dictionaries for JSON serialization\n",
    "#         final_data[\"chapter\"][i][\"characters\"] = [char.model_dump() for char in filled_data.characters]\n",
    "\n",
    "#     return final_data\n",
    "\n",
    "\n",
    "async def process_all_chapters_async(doc: fitz.Document, chapters: list, max_concurrency: int = 4) -> dict:\n",
    "    extracted = []\n",
    "    for ch in chapters:\n",
    "        text = extract_text_range(doc, ch[\"start_page\"], ch[\"end_page\"])\n",
    "        extracted.append({\"meta\": ch, \"text\": text})\n",
    "\n",
    "    final_data = make_chapter_skeletons(chapters)\n",
    "\n",
    "    aclient = AsyncOpenAI()\n",
    "    sem = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    tasks = []\n",
    "    index_map = []  \n",
    "    for i, item in enumerate(extracted):\n",
    "        if not item[\"text\"].strip():\n",
    "            continue\n",
    "        print(f\"Processing Chapter {item['meta']['chapter_id']}...\")\n",
    "        tasks.append(asyncio.create_task(\n",
    "            fill_chapter_with_model_async(aclient, item[\"text\"], semaphore=sem)\n",
    "        ))\n",
    "        index_map.append(i)\n",
    "\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    for task_idx, result in enumerate(results):\n",
    "        i = index_map[task_idx]\n",
    "        if isinstance(result, Exception):\n",
    "            # Leave defaults for this chapter (or you could add an \"error\" field)\n",
    "            print(f\"Chapter {final_data['chapter'][i]['chapter_id']} failed: {result}\")\n",
    "            continue\n",
    "\n",
    "        final_data[\"chapter\"][i][\"summary_local\"] = result.summary_local\n",
    "        final_data[\"chapter\"][i][\"characters\"] = [c.model_dump() for c in result.characters]\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4fd08",
   "metadata": {},
   "source": [
    "#### Look into parallel processing for chapter extraction , taking too long. ✅ (Done: 20 minutes to 4 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68501f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PROCESSING ALL CHAPTERS ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- PROCESSING ALL CHAPTERS ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# final_output = process_all_chapters(doc, chapters)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m final_output = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_all_chapters_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchapters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m output_filename = \u001b[33m\"\u001b[39m\u001b[33mfinal_story_output.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_filename, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abiju\\AppData\\Local\\anaconda3\\envs\\lc-academy-env\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# pdf_path = r\"C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Charlie and the Chocolate Factory (Roald Dahl).pdf\"\n",
    "# doc = fitz.open(pdf_path)\n",
    "\n",
    "# chapters = derive_chapter_ranges(doc)\n",
    "# if not chapters:\n",
    "#     print(\"No chapters were derived from the PDF's table of contents.\")\n",
    "# else:\n",
    "#     print(\"\\n--- PROCESSING ALL CHAPTERS ---\")\n",
    "#     # final_output = process_all_chapters(doc, chapters)\n",
    "#     final_output = asyncio.run(process_all_chapters_async(doc, chapters, max_concurrency=4))\n",
    "#     output_filename = \"final_story_output.json\"\n",
    "#     with open(output_filename, \"w\") as f:\n",
    "#         json.dump(final_output, f, indent=2)\n",
    "        \n",
    "#     print(f\"\\nProcessing complete. The final JSON object has been saved to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PROCESSING ALL CHAPTERS ---\n",
      "\n",
      "Processing complete. The final JSON object has been saved to 'final_story_output.json'\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"C:\\Users\\abiju\\Desktop\\Project-Velcro\\REference_textbook\\Charlie and the Chocolate Factory (Roald Dahl).pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "chapters = derive_chapter_ranges(doc)\n",
    "if not chapters:\n",
    "    print(\"No chapters were derived from the PDF's table of contents.\")\n",
    "else:\n",
    "    print(\"\\n--- PROCESSING ALL CHAPTERS ---\")\n",
    "    final_output = await process_all_chapters_async(doc, chapters, max_concurrency=4)\n",
    "\n",
    "    output_filename = \"final_story_output.json\"\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        json.dump(final_output, f, indent=2)\n",
    "\n",
    "    print(f\"\\nProcessing complete. The final JSON object has been saved to '{output_filename}'\")\n",
    "\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022d9ec",
   "metadata": {},
   "source": [
    "# Seconds pass : Now we connect between the chapters\n",
    "\n",
    "```jsonc\n",
    "{\n",
    "  \"chapter\": [{\n",
    "    \"chapter_id\": 0,                // Chapter number\n",
    "    \"title\": \"\",                    // Chapter title\n",
    "    \"pages\": [0, 0],                // [start_page, end_page]\n",
    "    \"summary_local\": \"\",            // <= 160 words \n",
    "    \"summary_global\": \"\",           // concatenated summary across chapters ≤ 250 words\n",
    "    \"characters\": [\n",
    "      {\n",
    "        \"name\": \"\",                 // Character name\n",
    "        \"aliases\": [],              // Concatenated aliases across all the chapters\n",
    "        \"status\": \"\",               // Update the character status as the story and relationships evolve\n",
    "        \"chapter_role\": \"\",         // Current role in the chapter in the context of the ongoing story \n",
    "        \"character_actions\": \"\",  // Key actions or events involving the character as the story is progressing (<= 150 words)\n",
    "        \"relationships\": [\n",
    "          {\n",
    "            \"with_name\": \"\",        // Other character's name\n",
    "            \"type\": \"\",             // Updated Relationship type as the story progresses and the relationships evolve\n",
    "            \"justification\": \"\",    // Updated in the context of the story <= 100 words\n",
    "            \"importance\": 0         // Updated importance level (0-5)\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  } \n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abc6de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "class RelationshipGlobal(BaseModel):\n",
    "    with_name: str\n",
    "    type: str\n",
    "    justification: str = Field(..., description=\"<= 100 words\")\n",
    "    importance: int = Field(..., description=\"0-5 scale\")\n",
    "\n",
    "class CharacterGlobal(BaseModel):\n",
    "    name: str\n",
    "    aliases: List[str]\n",
    "    status: str\n",
    "    chapter_role: str\n",
    "    character_actions: str = Field(..., description=\"Key actions or events involving the character in the ongoing story. Be descriptive and use short hand <= 200 words\")\n",
    "    relationships: List[RelationshipGlobal]\n",
    "\n",
    "class ChapterGlobal(BaseModel):\n",
    "    summary_global: str = Field(..., description=\"summary of the ongoing story <= 250 words\")\n",
    "    characters: List[CharacterGlobal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "931495a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_characters_locally(existing_chars: Dict[str, dict], new_chars: List[dict]) -> Dict[str, dict]:\n",
    "\n",
    "#     for char in new_chars:\n",
    "#         name = char['name']\n",
    "        \n",
    "#         matched_name = None\n",
    "#         for alias in char.get('aliases', []):\n",
    "#             for existing_name in existing_chars:\n",
    "#                 if alias == existing_name or alias in existing_chars[existing_name].get('aliases', []):\n",
    "#                     matched_name = existing_name\n",
    "#                     break\n",
    "#             if matched_name:\n",
    "#                 break\n",
    "        \n",
    "#         key = matched_name or name\n",
    "        \n",
    "#         if key in existing_chars:\n",
    "#             existing_aliases = set(existing_chars[key].get('aliases', []))\n",
    "#             new_aliases = set(char.get('aliases', []))\n",
    "#             existing_chars[key]['aliases'] = list(existing_aliases | new_aliases)\n",
    "            \n",
    "#             existing_chars[key]['status'] = char['status']\n",
    "#             existing_chars[key]['chapter_role'] = char['chapter_role']\n",
    "#             existing_chars[key]['character_actions'] = char['character_actions']\n",
    "#             existing_chars[key]['relationships'] = char['relationships']\n",
    "#         else:\n",
    "#             # Add new character\n",
    "#             existing_chars[key] = char.copy()\n",
    "    \n",
    "#     return existing_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dc3a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_fill_global = \"\"\"\n",
    "You are a library assistant who is skilled at extracting structured information from story book chapters. You are given the text of a chapter and must fill in the structured data fields, such that it meets the following criteria:\n",
    "1. The summary_global field must contain a concise summary of the ongoing story in the context of the previous chapter summary and current chapter summary provided. Even if you are aware about the story you are dealing with, do not add additional information that can potentially spoil the future chapters, limited to 250 words.\n",
    "2. The characters field must include all relevant characters introduced or developed in the chapter, along with their updated attributes. This includes :\n",
    "   - Name: Name of the character\n",
    "   - Aliases: List of character aliases\n",
    "   - Status: Current status of the character in the context of the ongoing story with reference to previous chapters and current chapter\n",
    "   - Chapter role: Role of the character in the chapter in the context of the ongoing story\n",
    "   - Character actions: Key actions or events involving the character in the ongoing story\n",
    "   - Relationships: List of relationships with other characters\n",
    "3. For the relationships field, it is a list of Relationship objects, each with the following fields:\n",
    "    - with_name: Other character's name\n",
    "    - type: Relationship type (e.g., ally | mentor | antagonist | family | rival | colleague | unknown)\n",
    "    - justification: an explanation in the context of the chapter why the relationship exists ( <= 100 words)\n",
    "    - importance: Importance of the relationship on a scale of 0-5, where 0 is negligible and 5 is critical to the story. Update the importance level as the story progresses.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98204cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_global_view(\n",
    "    client: AsyncOpenAI,\n",
    "    previous_summary: str,\n",
    "    previous_characters: dict,   # dict[str, dict] — your cumulative state\n",
    "    current_chapter: dict        # from first pass\n",
    ") :\n",
    "    context = {\n",
    "        \"previous_story_summary\": previous_summary,\n",
    "        \"all_known_characters_so_far\": list(previous_characters.values()),\n",
    "        \"current_chapter_title\": current_chapter[\"title\"],\n",
    "        \"current_chapter_summary\": current_chapter[\"summary_local\"],\n",
    "        \"characters_in_current_chapter\": current_chapter[\"characters\"],\n",
    "    }\n",
    "\n",
    "    resp = await client.responses.parse(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": chapter_fill_global},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(context, indent=2)}\n",
    "        ],\n",
    "        text_format=ChapterGlobal,\n",
    "    )\n",
    "    # Correctly returns the parsed Pydantic model\n",
    "    return resp.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_and_validate_global(ch: ChapterGlobal): # Essentially normalization\n",
    "    if not ch or not ch.characters:\n",
    "        return ch\n",
    "\n",
    "    for c in ch.characters:\n",
    "        # Normalize and deduplicate aliases\n",
    "        seen_aliases = set()\n",
    "        unique_aliases = []\n",
    "        for alias in (c.aliases or []):\n",
    "            stripped_alias = (alias or \"\").strip()\n",
    "            if stripped_alias and stripped_alias.lower() not in seen_aliases:\n",
    "                seen_aliases.add(stripped_alias.lower())\n",
    "                unique_aliases.append(stripped_alias)\n",
    "        c.aliases = unique_aliases\n",
    "\n",
    "        # Validate relationships\n",
    "        if not c.relationships:\n",
    "            continue\n",
    "        for r in c.relationships: # Fixing edge case hallucination\n",
    "            r.importance = max(0, min(5, int(r.importance or 0)))\n",
    "            if r.justification and len(r.justification.split()) > 100:\n",
    "                r.justification = \" \".join(r.justification.split()[:100]) + \"...\"\n",
    "    return ch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f84c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def second_pass_processing(client: AsyncOpenAI, first_pass_data: dict) -> dict:\n",
    "    chapters = first_pass_data[\"chapter\"]\n",
    "    final_output = {\"chapter\": []}\n",
    "\n",
    "    # Initialize cumulative state\n",
    "    cumulative_summary = \"\"\n",
    "    cumulative_characters = {}   # {canonical_name: character_dict}\n",
    "    alias_index = {}             # {normalized_alias: canonical_name}\n",
    "\n",
    "    for i, chapter_data in enumerate(chapters, start=1):\n",
    "        print(f\"Second Pass - Processing chapter {i}/{len(chapters)}: {chapter_data['title']}\")\n",
    "\n",
    "        # 1. Get the intelligently merged view from the LLM\n",
    "        global_view = await create_global_view(\n",
    "            client=client,\n",
    "            previous_summary=cumulative_summary,\n",
    "            previous_characters=cumulative_characters,\n",
    "            current_chapter=chapter_data\n",
    "        )\n",
    "\n",
    "        # 2. Apply guardrails: normalize and validate the LLM's output\n",
    "        validated_global_view = _normalize_and_validate_global(global_view)\n",
    "\n",
    "        # 3. Update the cumulative state using the *validated* data\n",
    "        cumulative_summary = validated_global_view.summary_global\n",
    "        \n",
    "        # Reset and rebuild the character dictionary and alias index from the new ground truth\n",
    "        cumulative_characters = {}\n",
    "        alias_index = {}\n",
    "        \n",
    "        for char_model in validated_global_view.characters:\n",
    "            char_dict = char_model.model_dump()\n",
    "            canonical_name = char_model.name\n",
    "            \n",
    "            cumulative_characters[canonical_name] = char_dict\n",
    "            \n",
    "            # Update the alias index for robust tracking\n",
    "            norm_canon_name = canonical_name.strip().casefold()\n",
    "            alias_index[norm_canon_name] = canonical_name\n",
    "            for alias in char_model.aliases:\n",
    "                alias_index[alias.strip().casefold()] = canonical_name\n",
    "\n",
    "        # 4. Append the state *as of this chapter* to the final output\n",
    "        final_output[\"chapter\"].append({\n",
    "            \"chapter_id\": chapter_data[\"chapter_id\"],\n",
    "            \"title\": chapter_data[\"title\"],\n",
    "            \"pages\": chapter_data[\"pages\"],\n",
    "            \"summary_local\": chapter_data[\"summary_local\"],\n",
    "            \"summary_global\": cumulative_summary,\n",
    "            \"characters\": list(cumulative_characters.values()),\n",
    "        })\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dbe6a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STARTING SECOND PASS PROCESSING ---\n",
      "Second Pass - Processing chapter 1/30: 1 Here Comes Charlie\n",
      "Second Pass - Processing chapter 2/30: 2 Mr Willy Wonka's Factory\n",
      "Second Pass - Processing chapter 3/30: 3 Mr Wonka and the Indian Prince\n",
      "Second Pass - Processing chapter 4/30: 4 The Secret Workers\n",
      "Second Pass - Processing chapter 5/30: 5 The Golden Tickets\n",
      "Second Pass - Processing chapter 6/30: 6 The First Two Finders\n",
      "Second Pass - Processing chapter 7/30: 7 Charlie's Birthday\n",
      "Second Pass - Processing chapter 8/30: 8 Two More Golden Tickets Found\n",
      "Second Pass - Processing chapter 9/30: 9 Grandpa Joe Takes a Gamble\n",
      "Second Pass - Processing chapter 10/30: 10 The Family Begins to Starve\n",
      "Second Pass - Processing chapter 11/30: 11 The Miracle\n",
      "Second Pass - Processing chapter 12/30: 12 What It Said on the Golden Ticket\n",
      "Second Pass - Processing chapter 13/30: 13 The Big Day Arrives\n",
      "Second Pass - Processing chapter 14/30: 14 Mr Willy Wonka\n",
      "Second Pass - Processing chapter 15/30: 15 The Chocolate Room\n",
      "Second Pass - Processing chapter 16/30: 16 The Oompa-Loompas\n",
      "Second Pass - Processing chapter 17/30: 17 Augustus Gloop Goes up the Pipe\n",
      "Second Pass - Processing chapter 18/30: 18 Down the Chocolate River\n",
      "Second Pass - Processing chapter 19/30: 19 The Inventing Room – Everlasting Gobstoppers and Hair Toffee\n",
      "Second Pass - Processing chapter 20/30: 20 The Great Gum Machine\n",
      "Second Pass - Processing chapter 21/30: 21 Good-bye Violet\n",
      "Second Pass - Processing chapter 22/30: 22 Along the Corridor\n",
      "Second Pass - Processing chapter 23/30: 23 Square Sweets That Look Round\n",
      "Second Pass - Processing chapter 24/30: 24 Veruca in the Nut Room\n",
      "Second Pass - Processing chapter 25/30: 25 The Great Glass Lift\n",
      "Second Pass - Processing chapter 26/30: 26 The Television-Chocolate Room\n",
      "Second Pass - Processing chapter 27/30: 27 Mike Teavee is Sent by Television\n",
      "Second Pass - Processing chapter 28/30: 28 Only Charlie Left\n",
      "Second Pass - Processing chapter 29/30: 29 The Other Children Go Home\n",
      "Second Pass - Processing chapter 30/30: 30 Charlie's Chocolate Factory\n",
      "\n",
      "Second pass complete! Saved to 'story_global_view.json'\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    try:\n",
    "        with open(\"final_story_output.json\", \"r\") as f:\n",
    "            first_pass_data = json.load(f)\n",
    "\n",
    "        client = AsyncOpenAI()\n",
    "\n",
    "        print(\"\\n--- STARTING SECOND PASS PROCESSING ---\")\n",
    "        second_pass_output = await second_pass_processing(client, first_pass_data)\n",
    "\n",
    "        output_filename = \"story_global_view.json\"\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            json.dump(second_pass_output, f, indent=2)\n",
    "\n",
    "        print(f\"\\nSecond pass complete! Saved to '{output_filename}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'final_story_output.json' not found. Please run the first pass script first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4c570",
   "metadata": {},
   "source": [
    "## TODO: Make a more efficient second pass: Fool around with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa738f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d0f6129",
   "metadata": {},
   "source": [
    "## Building RAG agent which will combine the JSON output we generated and use semantic search\n",
    "\n",
    "#### So this is going to be a chat interface between the user and the RAG agent. RAG agent will take user queries, take meta data about the current page / status of the chapter the user is currently in, and then use that information to generate queries to the underlying document store. It should be capable of generating sub queries to get more granular information from the document store and then stitch together the appropriate reponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b80ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "file_path = r\"C:\\Users\\abiju\\Desktop\\Project-Velcro\\REference_textbook\\Chocolate.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104eed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b59f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a18b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c824e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "page_content='Thereafter, just from chewing gum, \n",
      "Miss Bigelow was always dumb, \n",
      "And spent her life shut up in some \n",
      "Disgusting sanatorium. \n",
      "And that is why we'll try so hard \n",
      "To save Miss Violet Beauregarde \n",
      "From suffering an equal fate. \n",
      "She's still quite young. It's not too late, \n",
      "Provided she survives the cure. \n",
      "We hope she does. We can't be sure.' \n",
      "  \n",
      "22 \n",
      "Along the Corridor \n",
      " \n",
      "  'Well, well, well,' sighed Mr Willy Wonka, 'two naughty little children gone. Three good \n",
      "little children left. I think we'd better get out of this room quickly before we lose anyone else!' \n",
      "  'But Mr Wonka,' said Charlie Bucket anxiously, 'will Violet Beauregarde ever be all right \n",
      "again or will she always be a blueberry?' \n",
      "  'They'll de-juice her in no time flat!' declare d Mr Wonka. 'They'll roll her into the de-juicing \n",
      "machine, and she'll come out just as thin as a whistle!' \n",
      "  'But will she still be blue all over?' asked Charlie.' metadata={'producer': 'Acrobat Distiller 7.0 (Windows)', 'creator': 'Acrobat PDFMaker 7.0 for Word', 'creationdate': '2006-08-18T20:56:05-07:00', 'author': 'Aung Myo Min', 'moddate': '2006-08-18T20:56:18-07:00', 'company': 'MediaTech', 'sourcemodified': 'D:20060819035535', 'title': 'There are five children in this book:   AUGUSTUS GLOOP A greedy boy   VERUCA SALT A girl who is spoiled by her parents   VIOLET BEAUREGARDE A girl who chews gum all day long   MIKE TEAVEE A boy who does nothing but watch television   and   CHARLIE BUCKET', 'source': 'C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Chocolate.pdf', 'total_pages': 56, 'page': 37, 'page_label': '38', 'start_index': 0}\n",
      "*******************************************************************\n",
      "1\n",
      "page_content='at the girl. Mr Wonka was wringing his hands and saying, 'No, no, no, no, no! It isn't ready for \n",
      "eating! It isn't right! You mustn't do it!' \n",
      "  'Blueberry pie and cream!' shouted Violet. 'Here it comes! Oh my, it's perfect! It's beautiful! \n",
      "It's . . . it's exactly as though I'm swallowing it ! It's as though I'm chewing and swallowing great \n",
      "big spoonfuls of the most marvellous blueberry pie in the world!' \n",
      "  'Good heavens, girl!' shrieked Mrs Beau regarde suddenly, staring at Violet, 'what's \n",
      "happening to your nose!' \n",
      "  'Oh, be quiet, mother, and let me finish!' said Violet. \n",
      "  'It's turning blue!' screamed Mrs Beauregarde.  'Your nose is turning blue as a blueberry!' \n",
      "  'Your mother is right!' shouted Mr Beauregarde. 'Your whole nose has gone purple!' \n",
      "  'What do you mean?' said Violet, still chewing away.' metadata={'producer': 'Acrobat Distiller 7.0 (Windows)', 'creator': 'Acrobat PDFMaker 7.0 for Word', 'creationdate': '2006-08-18T20:56:05-07:00', 'author': 'Aung Myo Min', 'moddate': '2006-08-18T20:56:18-07:00', 'company': 'MediaTech', 'sourcemodified': 'D:20060819035535', 'title': 'There are five children in this book:   AUGUSTUS GLOOP A greedy boy   VERUCA SALT A girl who is spoiled by her parents   VIOLET BEAUREGARDE A girl who chews gum all day long   MIKE TEAVEE A boy who does nothing but watch television   and   CHARLIE BUCKET', 'source': 'C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Chocolate.pdf', 'total_pages': 56, 'page': 34, 'page_label': '35', 'start_index': 3175}\n",
      "*******************************************************************\n",
      "2\n",
      "page_content=''Your cheeks!' screamed Mrs Beauregarde. 'They' re turning blue as we ll! So is your chin! \n",
      "Your whole face is turning blue!' \n",
      "  'Spit that gum out at once!' ordered Mr Beauregarde. \n",
      "  'Mercy! Save us!' yelled Mrs Beauregarde. 'The  girl's going blue and purple all over! Even \n",
      "her hair is changing colour! Violet, you're turning violet, Violet! What is happening to you?' \n",
      "  'I told you I hadn't got it quite right, ' sighed Mr Wonka, shaking his head sadly. \n",
      "  'I'll say you haven't!' cried Mrs Beauregarde. 'Just look at the girl now!' \n",
      "  Everybody was staring at Violet. And what a terrible, peculiar sight she was! Her face and \n",
      "hands and legs and neck, in fact the skin all over her body, as we ll as her great big mop of curly \n",
      "hair, had turned a brilliant, purplish-blue, the colour of blueberry juice! \n",
      "  'It always goes wrong when we come to th e dessert,' sighed Mr Wonka. 'It's the blueberry \n",
      "pie that does it. But I'll get it right one day, you wait and see.'' metadata={'producer': 'Acrobat Distiller 7.0 (Windows)', 'creator': 'Acrobat PDFMaker 7.0 for Word', 'creationdate': '2006-08-18T20:56:05-07:00', 'author': 'Aung Myo Min', 'moddate': '2006-08-18T20:56:18-07:00', 'company': 'MediaTech', 'sourcemodified': 'D:20060819035535', 'title': 'There are five children in this book:   AUGUSTUS GLOOP A greedy boy   VERUCA SALT A girl who is spoiled by her parents   VIOLET BEAUREGARDE A girl who chews gum all day long   MIKE TEAVEE A boy who does nothing but watch television   and   CHARLIE BUCKET', 'source': 'C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Chocolate.pdf', 'total_pages': 56, 'page': 35, 'page_label': '36', 'start_index': 0}\n",
      "*******************************************************************\n",
      "3\n",
      "page_content=''It always goes wrong when we come to th e dessert,' sighed Mr Wonka. 'It's the blueberry \n",
      "pie that does it. But I'll get it right one day, you wait and see.' \n",
      "  'Violet,' screamed Mrs Beauregarde, 'you're swelling up!' \n",
      "  'I feel sick,' Violet said. \n",
      "'You're swelling up!' screamed Mrs Beauregarde again. \n",
      "  'I feel most peculiar!' gasped Violet. \n",
      "  'I'm not surprised!' said Mr Beauregarde. \n",
      "  'Great heavens, girl!' screeched Mrs Beauregarde. 'You're blowing up like a balloon!' \n",
      "  'Like a blueberry,' said Mr Wonka. \n",
      "  'Call a doctor!' shouted Mr Beauregarde. \n",
      "  'Prick her with a pin!' said one of the other fathers. \n",
      "  'Save her!' cried Mrs Beauregarde, wringing her hands. \n",
      "  But there was no saving her now. Her body was swelling up and chan ging shape at such a \n",
      "rate that within a minute it ha d turned into nothing less than an enormous round blue ball — a \n",
      "gigantic blueberry, in fact — and all that remained of Violet Beauregarde herself was a tiny pair of' metadata={'producer': 'Acrobat Distiller 7.0 (Windows)', 'creator': 'Acrobat PDFMaker 7.0 for Word', 'creationdate': '2006-08-18T20:56:05-07:00', 'author': 'Aung Myo Min', 'moddate': '2006-08-18T20:56:18-07:00', 'company': 'MediaTech', 'sourcemodified': 'D:20060819035535', 'title': 'There are five children in this book:   AUGUSTUS GLOOP A greedy boy   VERUCA SALT A girl who is spoiled by her parents   VIOLET BEAUREGARDE A girl who chews gum all day long   MIKE TEAVEE A boy who does nothing but watch television   and   CHARLIE BUCKET', 'source': 'C:\\\\Users\\\\abiju\\\\Desktop\\\\Project-Velcro\\\\REference_textbook\\\\Chocolate.pdf', 'total_pages': 56, 'page': 35, 'page_label': '36', 'start_index': 814}\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"Give me the song sung for Violet Beauregarde\"\n",
    ")\n",
    "\n",
    "i =0\n",
    "for r in results:\n",
    "    print(i)\n",
    "    i+=1\n",
    "    print(r)\n",
    "    print(\"*******************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57559b9e",
   "metadata": {},
   "source": [
    "## Building the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import MessagesState\n",
    "from typing import  Annotated\n",
    "import operator\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "''' \n",
    "Basic flow: User Query -> (Splits)-> Sematic Seartch & Current Chapter Sumamry and Relationships -> Library Assistant LLM (Reason) -> either Answer or Refine the Query (Ask for more context) -> (if refine, loop back to Search) -> Prodfuce Final Answer\n",
    "'''\n",
    "class SearchQuery(BaseModel):\n",
    "    query : str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "\n",
    "class StoryState(MessagesState):\n",
    "    curr_page: int \n",
    "    curr_chapter_id\n",
    "    max_num_turns: int # Number turns of research\n",
    "    context: Annotated[list, operator.add] # Source docs\n",
    "    interview: str # Interview transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff67050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: 'Who is Charlie and why is he important?'\n",
      "Found 10 results, filtered to 4 spoiler-free results\n",
      "Final Answer: Charlie is Charlie Bucket — one of the five children listed in the book and described as \"the hero.\" In this chapter he’s important because his family’s hopes rest on him: he got a Wonka bar for his birthday (but no Golden Ticket), two other tickets have just been claimed, and only one ticket remains unclaimed. The chapter stresses the Buckets’ quiet support for Charlie and raises the stakes for him as the possible finder of the final ticket.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from IPython.display import Image, display\n",
    "import operator\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "\n",
    "class StoryState(TypedDict):\n",
    "    messages: Annotated[List , operator.add]  # Conversation history\n",
    "    curr_page: int  # Reader's current page\n",
    "    curr_chapter_id: int  # Current chapter ID\n",
    "    iteration_count: int  # Track refinement iterations\n",
    "    max_iterations: int  # Maximum allowed iterations\n",
    "    question: str  # Current question being processed\n",
    "    chapter_context: dict  # Current chapter information\n",
    "    retrieved_docs: List[Document]  # Semantic search results\n",
    "\n",
    "# ============= Pydantic Models =============\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"Model for structured search query generation\"\"\"\n",
    "    query: str = Field(..., description=\"Refined search query\")\n",
    "\n",
    "# ============= Node Functions =============\n",
    "\n",
    "def initialize_question(state: StoryState) -> Dict:\n",
    "    \"\"\"Extract the initial question from the user's message\"\"\"\n",
    "    if state.get(\"messages\"):\n",
    "        question = state[\"messages\"][-1].content\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"iteration_count\": 0,\n",
    "            \"max_iterations\": 3  # Set default max iterations\n",
    "        }\n",
    "    return {}\n",
    "\n",
    "def get_current_chapter_context(state: StoryState) -> Dict:\n",
    "    \"\"\"Fetch the current chapter context based on reader's page\"\"\"\n",
    "    curr_page = state.get(\"curr_page\", 0)\n",
    "    \n",
    "    try:\n",
    "        with open(\"story_global_view.json\", \"r\") as f:\n",
    "            story_data = json.load(f)\n",
    "        \n",
    "        # Find the chapter containing the current page\n",
    "        for chapter in story_data.get(\"chapter\", []):\n",
    "            start_page, end_page = chapter.get(\"pages\", [-1, -1])\n",
    "            if start_page <= curr_page <= end_page:\n",
    "                return {\n",
    "                    \"chapter_context\": chapter,\n",
    "                    \"curr_chapter_id\": chapter.get(\"chapter_id\", 0)\n",
    "                }\n",
    "        \n",
    "        # Fallback to first chapter if no match\n",
    "        if story_data.get(\"chapter\"):\n",
    "            return {\n",
    "                \"chapter_context\": story_data[\"chapter\"][0],\n",
    "                \"curr_chapter_id\": story_data[\"chapter\"][0].get(\"chapter_id\", 1)\n",
    "            }\n",
    "    \n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Warning: Could not load story data: {e}\")\n",
    "    \n",
    "    return {\"chapter_context\": {}, \"curr_chapter_id\": 0}\n",
    "\n",
    "def semantic_search(state: StoryState) -> Dict:\n",
    "    query = state.get(\"question\", \"\")\n",
    "    curr_page = state.get(\"curr_page\", 0)\n",
    "    \n",
    "    if not query:\n",
    "        return {\"retrieved_docs\": []}\n",
    "    \n",
    "    # Determine the maximum allowed page (spoiler boundary)\n",
    "    max_allowed_page = -1\n",
    "    try:\n",
    "        with open(\"story_global_view.json\", \"r\") as f:\n",
    "            story_data = json.load(f)\n",
    "        \n",
    "        for chapter in story_data.get(\"chapter\", []):\n",
    "            start_page, end_page = chapter.get(\"pages\", [-1, -1])\n",
    "            if start_page <= curr_page <= end_page:\n",
    "                max_allowed_page = end_page\n",
    "                break\n",
    "    \n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Warning: Could not determine spoiler boundary: {e}\")\n",
    "        return {\"retrieved_docs\": []}\n",
    "    \n",
    "    if max_allowed_page == -1:\n",
    "        print(f\"Warning: Could not find chapter for page {curr_page}\")\n",
    "        return {\"retrieved_docs\": []}\n",
    "    \n",
    "    try:\n",
    "        unfiltered_results = vector_store.similarity_search(query, k=10)\n",
    "        \n",
    "        filtered_results = []\n",
    "        for doc in unfiltered_results:\n",
    "            doc_page = doc.metadata.get(\"page\")\n",
    "            if doc_page is not None and doc_page <= max_allowed_page:\n",
    "                filtered_results.append(doc)\n",
    "        \n",
    "        # Return top 4 filtered results\n",
    "        final_results = filtered_results[:4]\n",
    "        \n",
    "        print(f\"Search query: '{query}'\")\n",
    "        print(f\"Found {len(unfiltered_results)} results, filtered to {len(final_results)} spoiler-free results\")\n",
    "        \n",
    "        return {\"retrieved_docs\": final_results}\n",
    "    \n",
    "    except NameError:\n",
    "        print(\"Warning: vector_store not defined. Returning empty results.\")\n",
    "        return {\"retrieved_docs\": []}\n",
    "\n",
    "def generate_answer_or_refine(state: StoryState) -> Dict:\n",
    "    \n",
    "    chapter_context = state.get(\"chapter_context\", {})\n",
    "    retrieved_docs = state.get(\"retrieved_docs\", [])\n",
    "    question = state.get(\"question\", \"\")\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "    max_iterations = state.get(\"max_iterations\", 3)\n",
    "    \n",
    "    # Force final answer if we've hit max iterations\n",
    "    if iteration_count >= max_iterations:\n",
    "        print(f\"Max iterations ({max_iterations}) reached. Generating final answer.\")\n",
    "        return generate_final_answer(state)\n",
    "    \n",
    "    # Prepare context for the LLM\n",
    "    prompt = \"\"\"You are an expert library assistant specializing in helping readers understand stories without spoilers.\n",
    "\n",
    "**Current Chapter Information:**\n",
    "Title: {chapter_title}\n",
    "Summary: {chapter_summary}\n",
    "Characters: {chapter_characters}\n",
    "\n",
    "**Retrieved Information from Book:**\n",
    "{search_results}\n",
    "\n",
    "**Reader's Question:**\n",
    "{question}\n",
    "\n",
    "**Your Task:**\n",
    "Analyze if you have sufficient information to provide a complete, accurate answer.\n",
    "\n",
    "**Decision Rules:**\n",
    "1. If you can answer the question completely and accurately with the available information:\n",
    "   - Provide a clear, helpful response\n",
    "   - Base your answer ONLY on the provided context\n",
    "   - Never make up information\n",
    "\n",
    "2. If you need more specific information:\n",
    "   - Start your response with exactly: \"NEED_MORE_CONTEXT:\"\n",
    "   - Follow with a single, specific search query that would help find the missing information\n",
    "   - Make the query focused and relevant to what's missing\n",
    "\n",
    "**Remember:**\n",
    "- Never reference future chapters or events (spoilers)\n",
    "- Be conversational and helpful\n",
    "- If information is partially available, provide what you can\n",
    "\"\"\"\n",
    "    \n",
    "    # Format search results\n",
    "    search_results_text = \"\\n\\n\".join([\n",
    "        f\"[Page {doc.metadata.get('page', 'Unknown')}]: {doc.page_content[:300]}...\"\n",
    "        for doc in retrieved_docs\n",
    "    ]) if retrieved_docs else \"No relevant passages found in the book.\"\n",
    "    \n",
    "    # Format chapter information\n",
    "    chapter_title = chapter_context.get(\"title\", \"Unknown\")\n",
    "    chapter_summary = chapter_context.get(\"summary_global\", chapter_context.get(\"summary_local\", \"Not available\"))\n",
    "    \n",
    "    # Format characters (limit to avoid token overflow)\n",
    "    characters = chapter_context.get(\"characters\", [])\n",
    "    chapter_characters = \", \".join([\n",
    "        char.get(\"name\", \"Unknown\") for char in characters[:5]\n",
    "    ]) if characters else \"No characters listed\"\n",
    "    \n",
    "    # Invoke LLM\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=prompt.format(\n",
    "            chapter_title=chapter_title,\n",
    "            chapter_summary=chapter_summary,\n",
    "            chapter_characters=chapter_characters,\n",
    "            search_results=search_results_text,\n",
    "            question=question\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "        \"iteration_count\": iteration_count + 1\n",
    "    }\n",
    "\n",
    "def generate_final_answer(state: StoryState) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate the final answer when forced (max iterations reached)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"You are a helpful library assistant. Based on the available information, provide the best answer you can to the reader's question.\n",
    "\n",
    "**Available Information:**\n",
    "Chapter: {chapter_title}\n",
    "Summary: {chapter_summary}\n",
    "Retrieved passages: {search_results}\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "Provide a helpful response based ONLY on the available information. If some aspects cannot be answered, clearly state what you can and cannot answer.\n",
    "\"\"\"\n",
    "    \n",
    "    chapter_context = state.get(\"chapter_context\", {})\n",
    "    retrieved_docs = state.get(\"retrieved_docs\", [])\n",
    "    \n",
    "    search_results_text = \"\\n\".join([\n",
    "        f\"- {doc.page_content[:200]}...\" for doc in retrieved_docs[:3]\n",
    "    ]) if retrieved_docs else \"No specific passages available.\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=prompt.format(\n",
    "            chapter_title=chapter_context.get(\"title\", \"Unknown\"),\n",
    "            chapter_summary=chapter_context.get(\"summary_local\", \"Not available\"),\n",
    "            search_results=search_results_text,\n",
    "            question=state.get(\"question\", \"\")\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=f\"Based on the information available: {response.content}\")]}\n",
    "\n",
    "def extract_refined_query(state: StoryState) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract the refined query from the AI's response when more context is needed\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    if \"NEED_MORE_CONTEXT:\" in last_message:\n",
    "        # Extract the query after the marker\n",
    "        parts = last_message.split(\"NEED_MORE_CONTEXT:\", 1)\n",
    "        if len(parts) > 1:\n",
    "            new_query = parts[1].strip()\n",
    "            return {\"question\": new_query}\n",
    "    \n",
    "    # Fallback: use original question\n",
    "    return {}\n",
    "\n",
    "# ============= Routing Functions =============\n",
    "\n",
    "def decision_router(state: StoryState) -> str:\n",
    "    \"\"\"\n",
    "    Determine whether to refine the search or provide final answer\n",
    "    \"\"\"\n",
    "    if not state.get(\"messages\"):\n",
    "        return \"final_answer\"\n",
    "    \n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "    max_iterations = state.get(\"max_iterations\", 3)\n",
    "    \n",
    "    # Check if max iterations reached\n",
    "    if iteration_count >= max_iterations:\n",
    "        print(f\"Max iterations ({max_iterations}) reached. Routing to final answer.\")\n",
    "        return \"final_answer\"\n",
    "    \n",
    "    # Check if more context is needed\n",
    "    if \"NEED_MORE_CONTEXT:\" in last_message:\n",
    "        print(f\"Iteration {iteration_count}: Refining search\")\n",
    "        return \"refine_search\"\n",
    "    \n",
    "    return \"final_answer\"\n",
    "\n",
    "# ============= Graph Construction =============\n",
    "\n",
    "def build_interview_graph():\n",
    "    \"\"\"\n",
    "    Construct the optimized LangGraph RAG pipeline\n",
    "    \"\"\"\n",
    "    # Initialize the graph with our state\n",
    "    builder = StateGraph(StoryState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    builder.add_node(\"initialize_question\", initialize_question)\n",
    "    builder.add_node(\"get_chapter_context\", get_current_chapter_context)\n",
    "    builder.add_node(\"semantic_search\", semantic_search)\n",
    "    builder.add_node(\"generate_answer_or_refine\", generate_answer_or_refine)\n",
    "    builder.add_node(\"extract_refined_query\", extract_refined_query)\n",
    "    \n",
    "    # Define the flow\n",
    "    # 1. Start by initializing the question\n",
    "    builder.add_edge(START, \"initialize_question\")\n",
    "    \n",
    "    # 2. Then get chapter context (only once)\n",
    "    builder.add_edge(\"initialize_question\", \"get_chapter_context\")\n",
    "    \n",
    "    # 3. Perform initial semantic search\n",
    "    builder.add_edge(\"get_chapter_context\", \"semantic_search\")\n",
    "    \n",
    "    # 4. Generate answer or request refinement\n",
    "    builder.add_edge(\"semantic_search\", \"generate_answer_or_refine\")\n",
    "    \n",
    "    # 5. Conditional routing based on the response\n",
    "    builder.add_conditional_edges(\n",
    "        \"generate_answer_or_refine\",\n",
    "        decision_router,\n",
    "        {\n",
    "            \"refine_search\": \"extract_refined_query\",\n",
    "            \"final_answer\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 6. If refining, extract query and search again\n",
    "    builder.add_edge(\"extract_refined_query\", \"semantic_search\")\n",
    "    \n",
    "    # Compile with memory\n",
    "    memory = MemorySaver()\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# ============= Usage Example =============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Build the graph\n",
    "    interview_graph = build_interview_graph()\n",
    "    \n",
    "    # Visualize the graph\n",
    "    # try:\n",
    "    #     display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Could not display graph: {e}\")\n",
    "    display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "    # Example usage\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=\"Who is Charlie and why is he important?\")],\n",
    "        \"curr_page\": 42,  # Example: reader is on page 42\n",
    "        \"max_iterations\": 3\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    config = {\"configurable\": {\"thread_id\": \"story_thread_001\"}}\n",
    "    \n",
    "    # Note: This will fail without vector_store defined\n",
    "    # result = interview_graph.invoke(initial_state, config)\n",
    "    # print(\"Final Answer:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae7375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found chapter: 8 Two More Golden Tickets Found for page 42\n",
      "Spoiler boundary set to page 42\n",
      "Search query: 'What was the song sung for Violet?'\n",
      "Found 10 results, filtered to 4 spoiler-free results\n",
      "Iteration 1: Have content from 3 unique pages\n",
      "Answer provided, routing to end\n",
      "Assistant: The song is a short cautionary ditty warning about the fate of \"Miss Bigelow\" from chewing gum and urging that they try to save Violet Beauregarde. From the provided pages it includes lines like:\n",
      "\n",
      "- \"Thereafter, just from chewing gum, Miss Bigelow was always dumb,  \n",
      "And spent her life shut up in some  \n",
      "Disgusting sanatorium.\"\n",
      "\n",
      "and\n",
      "\n",
      "- \"And that is why we'll try so hard  \n",
      "To save Miss Violet Beauregarde  \n",
      "From suffering an equal fate.  \n",
      "She's still quite young. It's not too late,  \n",
      "Provided she survives the cur...\"\n",
      "\n",
      "I don't have the full lyrics or a title for the song in the pages you provided. Would you like me to look up the complete wording?\n",
      "\n",
      "Total unique documents retrieved: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from IPython.display import Image, display\n",
    "import operator\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "\n",
    "def deduplicate_docs(existing: List[Document], new: List[Document]) -> List[Document]:\n",
    "    if not existing:\n",
    "        return new\n",
    "    \n",
    "    seen = set()\n",
    "    for doc in existing:\n",
    "        page = doc.metadata.get(\"page\", -1)\n",
    "        # Use first 200 chars for hash to identify unique content\n",
    "        content_hash = hash(doc.page_content[:200]) if doc.page_content else 0\n",
    "        seen.add((page, content_hash))\n",
    "    \n",
    "    unique_new = []\n",
    "    for doc in new:\n",
    "        page = doc.metadata.get(\"page\", -1)\n",
    "        content_hash = hash(doc.page_content[:200]) if doc.page_content else 0\n",
    "        if (page, content_hash) not in seen:\n",
    "            unique_new.append(doc)\n",
    "            seen.add((page, content_hash))\n",
    "    \n",
    "    return existing + unique_new\n",
    "\n",
    "class StoryState(TypedDict):\n",
    "    messages: Annotated[List, operator.add]  \n",
    "    curr_page: int \n",
    "    curr_chapter_id: int  \n",
    "    iteration_count: int  \n",
    "    max_iterations: int  \n",
    "    question: str  \n",
    "    chapter_context: dict  \n",
    "    retrieved_docs: Annotated[List[Document], deduplicate_docs]  \n",
    "\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    query: str = Field(..., description=\"Clear, focused search query for finding information in the book\")\n",
    "\n",
    "\n",
    "\n",
    "def initialize_question(state: StoryState) -> Dict:\n",
    "    if state.get(\"messages\"):\n",
    "        for msg in reversed(state[\"messages\"]):\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                return {\n",
    "                    \"question\": msg.content,\n",
    "                    \"iteration_count\": 0,\n",
    "                    \"max_iterations\": state.get(\"max_iterations\", 3),\n",
    "                    \"retrieved_docs\": []  # Start fresh for new question\n",
    "                }\n",
    "    return {\n",
    "        \"iteration_count\": 0,\n",
    "        \"max_iterations\": 3\n",
    "    }\n",
    "\n",
    "def get_current_chapter_context(state: StoryState) -> Dict:\n",
    "    curr_page = state.get(\"curr_page\", 0)\n",
    "    \n",
    "\n",
    "    with open(\"story_global_view.json\", \"r\") as f:\n",
    "        story_data = json.load(f)\n",
    "    \n",
    "    chapters_list = story_data.get(\"chapter\", [])\n",
    "    \n",
    "    if not chapters_list:\n",
    "        print(\"Warning: No chapters found in story_global_view.json\")\n",
    "        return {\"chapter_context\": {}, \"curr_chapter_id\": 0}\n",
    "    \n",
    "    for chapter in chapters_list:\n",
    "        start_page, end_page = chapter.get(\"pages\", [-1, -1])\n",
    "        if start_page <= curr_page <= end_page:\n",
    "            print(f\"Found chapter: {chapter.get('title', 'Unknown')} for page {curr_page}\")\n",
    "            return {\n",
    "                \"chapter_context\": chapter,\n",
    "                \"curr_chapter_id\": chapter.get(\"chapter_id\", 0)\n",
    "            }\n",
    "    \n",
    "    print(f\"No chapter found for page {curr_page}, using first chapter\")\n",
    "    return {\n",
    "        \"chapter_context\": chapters_list[0],\n",
    "        \"curr_chapter_id\": chapters_list[0].get(\"chapter_id\", 1)\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "def semantic_search(state: StoryState) -> Dict:\n",
    "\n",
    "    query = state.get(\"question\", \"\")\n",
    "    curr_page = state.get(\"curr_page\", 0)\n",
    "    \n",
    "    if not query:\n",
    "        print(\"No query provided for semantic search\")\n",
    "        return {\"retrieved_docs\": []}\n",
    "    \n",
    "    max_allowed_page = -1\n",
    "    try:\n",
    "        with open(\"story_global_view.json\", \"r\") as f:\n",
    "            story_data = json.load(f)\n",
    "        \n",
    "        chapters_list = story_data.get(\"chapter\", [])\n",
    "        \n",
    "        for chapter in chapters_list:\n",
    "            start_page, end_page = chapter.get(\"pages\", [-1, -1])\n",
    "            if start_page <= curr_page <= end_page:\n",
    "                max_allowed_page = end_page\n",
    "                print(f\"Spoiler boundary set to page {max_allowed_page}\")\n",
    "                break\n",
    "    \n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Warning: Could not determine spoiler boundary: {e}\")\n",
    "        return {\"retrieved_docs\": []}\n",
    "    \n",
    "    if max_allowed_page == -1:\n",
    "        print(f\"Warning: Could not find chapter for page {curr_page}\")\n",
    "        return {\"retrieved_docs\": []}\n",
    "    \n",
    "\n",
    "    try:\n",
    "        unfiltered_results = vector_store.similarity_search(query, k=10)\n",
    "        \n",
    "        filtered_results = []\n",
    "        for doc in unfiltered_results:\n",
    "            doc_page = doc.metadata.get(\"page\")\n",
    "            if doc_page is not None and doc_page <= max_allowed_page:\n",
    "                filtered_results.append(doc)\n",
    "        \n",
    "        final_results = filtered_results[:4]\n",
    "        \n",
    "        print(f\"Search query: '{query}'\")\n",
    "        print(f\"Found {len(unfiltered_results)} results, filtered to {len(final_results)} spoiler-free results\")\n",
    "        \n",
    "        return {\"retrieved_docs\": final_results}\n",
    "    \n",
    "    except NameError:\n",
    "        print(\"Warning: vector_store not defined. Returning mock results for testing.\")\n",
    "        \n",
    "        mock_docs = [\n",
    "            Document(\n",
    "                page_content=f\"Mock content about {query[:30]}...\",\n",
    "                metadata={\"page\": curr_page - 1}\n",
    "            )\n",
    "        ]\n",
    "        return {\"retrieved_docs\": mock_docs}\n",
    "\n",
    "def generate_answer_or_refine(state: StoryState) -> Dict:\n",
    "    chapter_context = state.get(\"chapter_context\", {})\n",
    "    retrieved_docs = state.get(\"retrieved_docs\", [])\n",
    "    question = state.get(\"question\", \"\")\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "    max_iterations = state.get(\"max_iterations\", 3)\n",
    "    \n",
    "    if iteration_count >= max_iterations:\n",
    "        print(f\"Max iterations ({max_iterations}) reached. Generating final answer.\")\n",
    "        return generate_final_answer(state)\n",
    "    \n",
    "    search_results_text = \"\\n\\n\".join([\n",
    "        f\"[Page {doc.metadata.get('page', 'Unknown')}]: {doc.page_content[:300]}...\"\n",
    "        for doc in retrieved_docs\n",
    "    ]) if retrieved_docs else \"No relevant passages found in the book.\"\n",
    "    \n",
    "\n",
    "    unique_pages = set(doc.metadata.get('page', -1) for doc in retrieved_docs if doc.metadata.get('page'))\n",
    "    print(f\"Iteration {iteration_count + 1}: Have content from {len(unique_pages)} unique pages\")\n",
    "    \n",
    "    # Prepare context for the LLM\n",
    "    prompt = \"\"\"You are an expert library assistant specializing in helping readers understand stories without spoilers.\n",
    "\n",
    "**Current Chapter Information:**\n",
    "Title: {chapter_title}\n",
    "Summary: {chapter_summary}\n",
    "Characters in this chapter: {chapter_characters}\n",
    "\n",
    "**Retrieved Information from Book ({num_pages} unique pages):**\n",
    "{search_results}\n",
    "\n",
    "**Reader's Question:**\n",
    "{question}\n",
    "\n",
    "**Your Task:**\n",
    "Analyze if you have sufficient information to provide a complete, accurate answer.\n",
    "\n",
    "**Decision Rules:**\n",
    "1. If you can answer the question completely and accurately with the available information:\n",
    "   - Provide a clear, helpful response\n",
    "   - Base your answer ONLY on the provided context\n",
    "   - Never make up information\n",
    "   - Be conversational and helpful\n",
    "\n",
    "2. If you need more specific information:\n",
    "   - Start your response with exactly: \"NEED_MORE_CONTEXT:\"\n",
    "   - Follow immediately with a specific, focused search query\n",
    "   - The query should target the missing information precisely\n",
    "   - Keep the query under 10 words and very focused\n",
    "\n",
    "**Remember:**\n",
    "- Never reference future chapters or events (spoilers)\n",
    "- It is better to say \"I don't have that information\" than to guess\n",
    "- If you have partial information, share what you know and note what's missing\n",
    "\"\"\"\n",
    "    \n",
    "    # Format chapter information\n",
    "    chapter_title = chapter_context.get(\"title\", \"Unknown\")\n",
    "    chapter_summary = chapter_context.get(\"summary_global\", \n",
    "                                         chapter_context.get(\"summary_local\", \"Not available\"))\n",
    "    \n",
    "    # Format characters (limit to avoid token overflow)\n",
    "    characters = chapter_context.get(\"characters\", [])\n",
    "    if characters:\n",
    "        character_names = [char.get(\"name\", \"Unknown\") for char in characters[:5]]\n",
    "        chapter_characters = \", \".join(character_names)\n",
    "        if len(characters) > 5:\n",
    "            chapter_characters += f\" (and {len(characters) - 5} more)\"\n",
    "    else:\n",
    "        chapter_characters = \"No characters listed\"\n",
    "    \n",
    "    # Invoke LLM\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=prompt.format(\n",
    "            chapter_title=chapter_title,\n",
    "            chapter_summary=chapter_summary,\n",
    "            chapter_characters=chapter_characters,\n",
    "            search_results=search_results_text,\n",
    "            question=question,\n",
    "            num_pages=len(unique_pages)\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "        \"iteration_count\": iteration_count + 1\n",
    "    }\n",
    "\n",
    "def generate_final_answer(state: StoryState):\n",
    "    \"\"\"\n",
    "    Generate the final answer when forced (max iterations reached).\n",
    "    This ensures we always provide something useful to the reader.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"You are a helpful library assistant. Based on all the information gathered, provide the best answer you can.\n",
    "\n",
    "**Available Information:**\n",
    "Chapter: {chapter_title}\n",
    "Summary: {chapter_summary}\n",
    "\n",
    "**All Retrieved Passages ({num_docs} documents):**\n",
    "{search_results}\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "Provide a helpful response based ONLY on the available information. \n",
    "- If you can partially answer, do so and note what aspects remain unclear\n",
    "- If you cannot answer at all, explain what information would be needed\n",
    "- Never make up information\n",
    "\"\"\"\n",
    "    \n",
    "    chapter_context = state.get(\"chapter_context\", {})\n",
    "    retrieved_docs = state.get(\"retrieved_docs\", [])\n",
    "    \n",
    "    # Include ALL accumulated documents (since we deduplicated)\n",
    "    search_results_text = \"\\n\\n\".join([\n",
    "        f\"[Page {doc.metadata.get('page', 'Unknown')}]: {doc.page_content[:250]}...\"\n",
    "        for doc in retrieved_docs\n",
    "    ]) if retrieved_docs else \"No specific passages available.\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=prompt.format(\n",
    "            chapter_title=chapter_context.get(\"title\", \"Unknown\"),\n",
    "            chapter_summary=chapter_context.get(\"summary_local\", \"Not available\"),\n",
    "            search_results=search_results_text,\n",
    "            question=state.get(\"question\", \"\"),\n",
    "            num_docs=len(retrieved_docs)\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    final_message = f\"Based on the information available:\\n\\n{response.content}\"\n",
    "    return {\"messages\": [AIMessage(content=final_message)]}\n",
    "\n",
    "def extract_refined_query(state: StoryState) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract and validate refined query using structured output.\n",
    "    This ensures we get a properly formatted search query.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    if \"NEED_MORE_CONTEXT:\" not in last_message:\n",
    "        print(\"Warning: No refinement marker found in message\")\n",
    "        return {}\n",
    "    \n",
    "    # Extract the text after the marker\n",
    "    parts = last_message.split(\"NEED_MORE_CONTEXT:\", 1)\n",
    "    if len(parts) < 2:\n",
    "        return {}\n",
    "    \n",
    "    query_text = parts[1].strip()\n",
    "    \n",
    "    # Use structured output to ensure we get a clean, focused query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    \n",
    "    try:\n",
    "        # Clean and validate the query\n",
    "        clean_query_prompt = \"\"\"Extract a clear, focused search query from this text. \n",
    "        The query should be:\n",
    "        - Under 10 words\n",
    "        - Specific and targeted\n",
    "        - Suitable for semantic search\n",
    "        \n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        \n",
    "        refined_query = structured_llm.invoke([\n",
    "            SystemMessage(content=clean_query_prompt.format(text=query_text))\n",
    "        ])\n",
    "        \n",
    "        print(f\"Refined query: '{refined_query.query}'\")\n",
    "        return {\"question\": refined_query.query}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting refined query: {e}\")\n",
    "        # Fallback to raw text if structured output fails\n",
    "        return {\"question\": query_text[:50]}  # Limit length\n",
    "\n",
    "\n",
    "def decision_router(state: StoryState) -> str:\n",
    "\n",
    "    if not state.get(\"messages\"):\n",
    "        return \"final_answer\"\n",
    "    \n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "    max_iterations = state.get(\"max_iterations\", 3)\n",
    "    \n",
    "    # Check if max iterations reached\n",
    "    if iteration_count >= max_iterations:\n",
    "        print(f\"Max iterations ({max_iterations}) reached. Routing to final answer.\")\n",
    "        return \"final_answer\"\n",
    "    \n",
    "    # Check if more context is needed\n",
    "    if \"NEED_MORE_CONTEXT:\" in last_message:\n",
    "        print(f\"Iteration {iteration_count}: Needs refinement, routing to extract query\")\n",
    "        return \"refine_search\"\n",
    "    \n",
    "    print(\"Answer provided, routing to end\")\n",
    "    return \"final_answer\"\n",
    "\n",
    "\n",
    "def build_interview_graph():\n",
    "    \"\"\"\n",
    "    Construct the optimized LangGraph RAG pipeline with proper accumulation.\n",
    "    \"\"\"\n",
    "    # Initialize the graph with our state\n",
    "    builder = StateGraph(StoryState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    builder.add_node(\"initialize_question\", initialize_question)\n",
    "    builder.add_node(\"get_chapter_context\", get_current_chapter_context)\n",
    "    builder.add_node(\"semantic_search\", semantic_search)\n",
    "    builder.add_node(\"generate_answer_or_refine\", generate_answer_or_refine)\n",
    "    builder.add_node(\"extract_refined_query\", extract_refined_query)\n",
    "    \n",
    "    # Define the flow\n",
    "    # 1. Start by initializing the question\n",
    "    builder.add_edge(START, \"initialize_question\")\n",
    "    \n",
    "    # 2. Then get chapter context (only once per conversation)\n",
    "    builder.add_edge(\"initialize_question\", \"get_chapter_context\")\n",
    "    \n",
    "    # 3. Perform initial semantic search\n",
    "    builder.add_edge(\"get_chapter_context\", \"semantic_search\")\n",
    "    \n",
    "    # 4. Generate answer or request refinement\n",
    "    builder.add_edge(\"semantic_search\", \"generate_answer_or_refine\")\n",
    "    \n",
    "    # 5. Conditional routing based on the response\n",
    "    builder.add_conditional_edges(\n",
    "        \"generate_answer_or_refine\",\n",
    "        decision_router,\n",
    "        {\n",
    "            \"refine_search\": \"extract_refined_query\",\n",
    "            \"final_answer\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 6. If refining, extract query and search again\n",
    "    builder.add_edge(\"extract_refined_query\", \"semantic_search\")\n",
    "    \n",
    "    # Compile with memory\n",
    "    memory = MemorySaver()\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Build the graph\n",
    "    interview_graph = build_interview_graph()\n",
    "    \n",
    "    # Visualize the graph\n",
    "    try:\n",
    "        display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display graph: {e}\")\n",
    "    \n",
    "    # Example usage\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=\"W\")],\n",
    "        \"curr_page\": 42,  # Example: reader is on page 42\n",
    "        \"max_iterations\": 3\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    config = {\"configurable\": {\"thread_id\": \"story_thread_001\"}}\n",
    "    \n",
    "    try:\n",
    "        # Note: This requires vector_store to be defined\n",
    "        result = interview_graph.invoke(initial_state, config)\n",
    "        \n",
    "        # Print final answer\n",
    "        for msg in result[\"messages\"]:\n",
    "            if isinstance(msg, AIMessage):\n",
    "                print(\"Assistant:\", msg.content)\n",
    "                \n",
    "        # Show accumulated documents\n",
    "        print(f\"\\nTotal unique documents retrieved: {len(result.get('retrieved_docs', []))}\")\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"Note: {e}\")\n",
    "        print(\"Define vector_store before running the full pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31e395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
